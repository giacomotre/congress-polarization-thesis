{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as CumlTfidfVectorizer\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_output_dir = Path(\"data/vocabulary_dumps\") # Base directory for vocabulary files\n",
    "base_dir = Path(\"../data/merged\")\n",
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "base_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cuml_vocab_parquet_path = base_output_dir / \"global_vocabulary_cuml.parquet\"\n",
    "sklearn_vocab_dict_path = base_output_dir / \"global_vocabulary_sklearn.joblib\"\n",
    "\n",
    "all_speeches_list_cpu = []\n",
    "\n",
    "print(\"Loading and aggregating speeches from all years (CPU)...\")\n",
    "\n",
    "for i in range(75, 112):\n",
    "    \n",
    "    \"------ Loading files -------\"\n",
    "    print(f'── Processing Congress {i} ──')\n",
    "    year_str = f\"{i:03}\"\n",
    "    house_file = base_dir / f\"house_db/house_merged_{year_str}.csv\"\n",
    "    if base_dir.exists():\n",
    "        try:\n",
    "            df_vocab = pd.read_csv(house_file, usecols=['speech'])\n",
    "            df_vocab.dropna(subset=['speech'], inplace=True)\n",
    "            all_speeches_list_cpu.extend(df_vocab['speech'].astype(str).tolist()) # Ensure speech is string\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {house_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found for vocab generation: {house_file}\")\n",
    "\n",
    "if not all_speeches_list_cpu:\n",
    "    print(\"Error: No speeches collected for vocabulary generation. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Collected {len(all_speeches_list_cpu)} speeches.\")\n",
    "\n",
    "# --- 2. Transfer to GPU (CPU -> GPU) ---\n",
    "print(\"Transferring speeches to GPU...\")\n",
    "try:\n",
    "    all_speeches_cudf_series = cudf.Series(all_speeches_list_cpu)\n",
    "    del all_speeches_list_cpu # Free CPU memory\n",
    "except Exception as e:\n",
    "    print(f\"Error converting list to cudf.Series: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Vocabulary Computation (GPU) using cuML ---\n",
    "print(\"Building global vocabulary with cuML TfidfVectorizer (GPU)...\")\n",
    "try:\n",
    "    vocab_builder_vectorizer_cuml = CumlTfidfVectorizer(\n",
    "        min_df=200,\n",
    "        ngram_range=(1, 1) # Unigrams\n",
    "        # cuML's TfidfVectorizer doesn't have a direct 'stop_words=None' like sklearn;\n",
    "        # its default behavior is typically no stop word removal unless a list is provided.\n",
    "    )\n",
    "    vocab_builder_vectorizer_cuml.fit(all_speeches_cudf_series)\n",
    "    del all_speeches_cudf_series # Free GPU memory\n",
    "except Exception as e:\n",
    "    print(f\"Error during cuML TfidfVectorizer fitting: {e}\")\n",
    "    exit()\n",
    "\n",
    "# The vocabulary_ attribute in cuML's fitted TfidfVectorizer is a cudf.Series of the terms,\n",
    "# sorted alphabetically (this is typical for cuML's vocabulary handling).\n",
    "gpu_terms_cudf_series = vocab_builder_vectorizer_cuml.vocabulary_\n",
    "print(f\"Generated global vocabulary with {len(gpu_terms_cudf_series)} features on GPU.\")\n",
    "\n",
    "# --- 4. Create and Save Dumps ---\n",
    "\n",
    "# 4a. For cuML: Save the cudf.Series of terms to a Parquet file\n",
    "print(f\"Saving cuML vocabulary (cudf.Series of terms) to {cuml_vocab_parquet_path}...\")\n",
    "try:\n",
    "    # To save a Series to Parquet, it's often easiest to convert it to a DataFrame with one column\n",
    "    gpu_terms_cudf_series.to_frame(name='term').to_parquet(cuml_vocab_parquet_path)\n",
    "    print(f\"Successfully saved cuML vocabulary to {cuml_vocab_parquet_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving cuML vocabulary to Parquet: {e}\")\n",
    "\n",
    "# 4b. For Scikit-learn: Create a dictionary {'term': index} and save with joblib\n",
    "print(f\"Creating and saving scikit-learn vocabulary (dictionary) to {sklearn_vocab_dict_path}...\")\n",
    "try:\n",
    "    # Convert the cuDF Series of terms (on GPU) to a Python list (on CPU)\n",
    "    # The order in gpu_terms_cudf_series determines the indices.\n",
    "    cpu_terms_list = gpu_terms_cudf_series.to_pandas().tolist()\n",
    "    \n",
    "    # Create the scikit-learn style vocabulary dictionary\n",
    "    sklearn_vocab_dict = {term: idx for idx, term in enumerate(cpu_terms_list)}\n",
    "    \n",
    "    joblib.dump(sklearn_vocab_dict, sklearn_vocab_dict_path)\n",
    "    print(f\"Successfully saved scikit-learn vocabulary dictionary to {sklearn_vocab_dict_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating/saving scikit-learn vocabulary dictionary: {e}\")\n",
    "\n",
    "print(\"\\n--- Vocabulary Generation Finished ---\")\n",
    "print(f\"cuML vocabulary (Parquet): {cuml_vocab_parquet_path}\")\n",
    "print(f\"Scikit-learn vocabulary (joblib dict): {sklearn_vocab_dict_path}\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "del vocab_builder_vectorizer_cuml\n",
    "if 'gpu_terms_cudf_series' in locals(): del gpu_terms_cudf_series\n",
    "if 'sklearn_vocab_dict' in locals(): del sklearn_vocab_dict\n",
    "if 'cpu_terms_list' in locals(): del cpu_terms_list\n",
    "\n",
    "cupy.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f25579",
   "metadata": {},
   "source": [
    "# CPU version (actual used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re # Using regex for more flexible searching\n",
    "import joblib # For saving the scikit-learn dictionary\n",
    "\n",
    "# Import scikit-learn's TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as SklearnTfidfVectorizer\n",
    "\n",
    "# cudf is needed only if you want to save the cuML-compatible vocab as Parquet\n",
    "# If you don't have cuDF in the environment where this CPU script runs,\n",
    "# you could save the cpu_terms_list as a simple text file or CSV instead for cuML.\n",
    "# For now, I'll assume cuDF might be available or you'll adapt the cuML saving part.\n",
    "try:\n",
    "    import cudf\n",
    "    CUDF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CUDF_AVAILABLE = False\n",
    "    print(\"Warning: cudf not found. Will not be able to save cuML vocabulary in Parquet format directly from this script.\")\n",
    "    print(\"Consider saving as a text file or CSV instead for the cuML vocabulary list.\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_OUTPUT_DIR = Path(\"/content/drive/MyDrive/congress-polarization-thesis/data/vocabulary_dumps\") # Base directory for vocabulary files\n",
    "BASE_DIR = Path(\"/content/drive/MyDrive/congress-polarization-thesis/data/processed\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define output paths\n",
    "CUML_VOCAB_OUTPUT_PATH = BASE_OUTPUT_DIR / \"global_vocabulary_processed_v2_200_min_df_cuml_from_sklearn.parquet\" # Changed name slightly for clarity\n",
    "SKLEARN_VOCAB_DICT_PATH = BASE_OUTPUT_DIR / \"global_vocabulary_processed_v2_200_min_df_sklearn_from_sklearn.joblib\" # Changed name slightly\n",
    "\n",
    "all_speeches_list_cpu = []\n",
    "GLOBAL_MIN_DF = 200 # Your min_df setting\n",
    "TOKEN_PATTERN_WORDS_ONLY = r\"(?u)\\b[a-zA-Z]{2,}\\b\" # Stricter token pattern\n",
    "\n",
    "print(\"Loading and aggregating speeches from all years (CPU)...\")\n",
    "\n",
    "for i in range(76, 112): # Your full range\n",
    "    print(f'── Processing Congress {i} ──')\n",
    "    year_str = f\"{i:03}\"\n",
    "    house_file = BASE_DIR / f\"house_db/house_cleaned_{year_str}.csv\" # Reading CLEANED files\n",
    "\n",
    "    # Path check was: if base_dir.exists(): which checks the directory, not the file\n",
    "    # Should be: if house_file.exists():\n",
    "    if house_file.exists():\n",
    "        try:\n",
    "            df_vocab = pd.read_csv(house_file, usecols=['speech'])\n",
    "            df_vocab.dropna(subset=['speech'], inplace=True)\n",
    "            all_speeches_list_cpu.extend(df_vocab['speech'].astype(str).tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {house_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found for vocab generation: {house_file}\")\n",
    "\n",
    "if not all_speeches_list_cpu:\n",
    "    print(\"Error: No speeches collected for vocabulary generation. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Collected {len(all_speeches_list_cpu)} speeches.\")\n",
    "\n",
    "#------ Debugging: Inspect Speeches (as before) ------\n",
    "print(\"\\n--- Inspecting First 5 Speeches ---\")\n",
    "for i_debug, speech_debug in enumerate(all_speeches_list_cpu[:5]):\n",
    "    print(f\"Speech {i_debug}: {speech_debug[:200]}...\")\n",
    "    print(f\"  Type: {type(speech_debug)}\")\n",
    "    print(f\"  Length (chars): {len(speech_debug)}\")\n",
    "print(\"--------------------------------\\n\")\n",
    "\n",
    "print(\"\\n--- Checking for long 'words' ---\")\n",
    "long_words = []\n",
    "for speech_debug in all_speeches_list_cpu[:10000]:\n",
    "    if isinstance(speech_debug, str):\n",
    "        words = speech_debug.split() # Simple split for this check\n",
    "        for word in words:\n",
    "            if len(word) > 50:\n",
    "                long_words.append(word)\n",
    "if long_words:\n",
    "    print(f\"Found {len(long_words)} 'words' longer than 50 chars. Examples: {long_words[:10]}\")\n",
    "else:\n",
    "    print(\"No unusually long 'words' found in sample (based on simple split).\")\n",
    "print(\"-----------------------------\\n\")\n",
    "#-------- End Debugging Speeches --------\n",
    "\n",
    "# --- Vocabulary Computation (CPU) using Scikit-learn ---\n",
    "print(\"Building global vocabulary with Scikit-learn TfidfVectorizer (CPU)...\")\n",
    "try:\n",
    "    vocab_builder_vectorizer_sklearn = SklearnTfidfVectorizer(\n",
    "        min_df=GLOBAL_MIN_DF,\n",
    "        ngram_range=(1, 2),        # Unigrams\n",
    "        #token_pattern=TOKEN_PATTERN_WORDS_ONLY, # Use the stricter pattern\n",
    "        lowercase=False,           # False, Assuming already lowercased by SpaCy\n",
    "        stop_words=None            # Assuming stop words removed by SpaCy\n",
    "    )\n",
    "\n",
    "    # Fit on the CPU list of speeches\n",
    "    print(f\"Fitting SklearnTfidfVectorizer on {len(all_speeches_list_cpu)} speeches...\")\n",
    "    vocab_builder_vectorizer_sklearn.fit(all_speeches_list_cpu)\n",
    "\n",
    "    # Get the vocabulary dictionary for scikit-learn\n",
    "    sklearn_vocab_dict = vocab_builder_vectorizer_sklearn.vocabulary_\n",
    "\n",
    "    # Get the feature names (terms) in the order of their indices\n",
    "    # This list will be used for the cuML-compatible output\n",
    "    cpu_terms_list = vocab_builder_vectorizer_sklearn.get_feature_names_out().tolist()\n",
    "\n",
    "    print(f\"Generated global vocabulary with {len(cpu_terms_list)} features using scikit-learn.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Scikit-learn TfidfVectorizer fitting: {e}\")\n",
    "    # Consider exiting or handling more gracefully\n",
    "    sklearn_vocab_dict = {}\n",
    "    cpu_terms_list = []\n",
    "    # exit() # Optional: exit if vocab building fails\n",
    "\n",
    "#--- Debugging: Inspect Vocabulary (as before) ---\n",
    "if cpu_terms_list:\n",
    "    print(\"\\n--- Inspecting First 100 Vocabulary Terms (from scikit-learn) ---\")\n",
    "    print(cpu_terms_list[:100])\n",
    "    print(\"-----------------------------------------------------------------\\n\")\n",
    "# --- End Debugging Vocabulary ---\n",
    "\n",
    "\n",
    "# --- Create and Save Dumps ---\n",
    "\n",
    "# 1. For Scikit-learn: Save the dictionary {'term': index}\n",
    "print(f\"Creating and saving scikit-learn vocabulary (dictionary) to {SKLEARN_VOCAB_DICT_PATH}...\")\n",
    "if sklearn_vocab_dict:\n",
    "    try:\n",
    "        joblib.dump(sklearn_vocab_dict, SKLEARN_VOCAB_DICT_PATH)\n",
    "        print(f\"Successfully saved scikit-learn vocabulary dictionary to {SKLEARN_VOCAB_DICT_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating/saving scikit-learn vocabulary dictionary: {e}\")\n",
    "else:\n",
    "    print(\"Scikit-learn vocabulary dictionary is empty, not saving.\")\n",
    "\n",
    "\n",
    "# 2. For cuML: Save the list of terms (cpu_terms_list)\n",
    "#    Option A: As Parquet using cuDF (if available)\n",
    "#    Option B: As a simple text file (one term per line) if cuDF is not available\n",
    "print(f\"Saving cuML-compatible vocabulary (list of terms)...\")\n",
    "if cpu_terms_list:\n",
    "    if CUDF_AVAILABLE:\n",
    "        print(f\"Attempting to save as Parquet to {CUML_VOCAB_OUTPUT_PATH} using cuDF...\")\n",
    "        try:\n",
    "            terms_cudf_series = cudf.Series(cpu_terms_list)\n",
    "            terms_cudf_series.to_frame(name='term').to_parquet(CUML_VOCAB_OUTPUT_PATH)\n",
    "            print(f\"Successfully saved cuML-compatible vocabulary (Parquet) to {CUML_VOCAB_OUTPUT_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cuML-compatible vocabulary to Parquet: {e}\")\n",
    "            print(\"Consider saving as a text file as a fallback if Parquet fails.\")\n",
    "            CUDF_AVAILABLE = False # Fallback to text if save fails\n",
    "\n",
    "    if not CUDF_AVAILABLE: # If cuDF wasn't available or Parquet saving failed\n",
    "        cuml_vocab_text_path = BASE_OUTPUT_DIR / \"global_vocabulary_cuml_from_sklearn.txt\"\n",
    "        print(f\"Saving cuML-compatible vocabulary as text file to {cuml_vocab_text_path}...\")\n",
    "        try:\n",
    "            with open(cuml_vocab_text_path, 'w', encoding='utf-8') as f:\n",
    "                for term in cpu_terms_list:\n",
    "                    f.write(term + \"\\n\")\n",
    "            print(f\"Successfully saved cuML-compatible vocabulary (text file) to {cuml_vocab_text_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cuML-compatible vocabulary to text file: {e}\")\n",
    "else:\n",
    "    print(\"CPU terms list is empty, not saving cuML-compatible vocabulary.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Vocabulary Generation Finished ---\")\n",
    "if sklearn_vocab_dict:\n",
    "    print(f\"Scikit-learn vocabulary (joblib dict): {SKLEARN_VOCAB_DICT_PATH}\")\n",
    "if cpu_terms_list:\n",
    "    if CUDF_AVAILABLE and CUML_VOCAB_OUTPUT_PATH.exists():\n",
    "         print(f\"cuML vocabulary (Parquet from scikit-learn terms): {CUML_VOCAB_OUTPUT_PATH}\")\n",
    "    elif not CUDF_AVAILABLE and (BASE_OUTPUT_DIR / \"global_vocabulary_cuml_from_sklearn.txt\").exists():\n",
    "        print(f\"cuML vocabulary (Text file from scikit-learn terms): {BASE_OUTPUT_DIR / 'global_vocabulary_cuml_from_sklearn.txt'}\")\n",
    "\n",
    "\n",
    "# --- Cleanup (CPU based, so no cupy cleanup needed) ---\n",
    "if 'vocab_builder_vectorizer_sklearn' in locals(): del vocab_builder_vectorizer_sklearn\n",
    "if 'sklearn_vocab_dict' in locals(): del sklearn_vocab_dict\n",
    "if 'cpu_terms_list' in locals(): del cpu_terms_list\n",
    "if 'all_speeches_list_cpu' in locals(): del all_speeches_list_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ed9db",
   "metadata": {},
   "source": [
    "vocabulary dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb727a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking vocabulary sizes...\n",
      "\n",
      "✓ Scikit-learn vocabulary (joblib): 235,657 words\n",
      "  Sample words: ['speaker', 'direction', 'authority', 'republican', 'conference', 'represent', 'minority', 'party', 'nominate', 'office']\n",
      "\n",
      "✓ cuML vocabulary (Parquet): 235,657 words\n",
      "  Sample terms: ['aa', 'ab', 'aback', 'abandon', 'abandon child', 'abandon commitment', 'abandon effort', 'abandon hazardous', 'abandon hope', 'abandon land']\n",
      "\n",
      "✗ cuML vocabulary (Text) file not found at: ..\\data\\vocabulary_dumps\\1_word\\global_vocabulary_cuml_from_sklearn.txt\n",
      "\n",
      "==================================================\n",
      "Summary:\n",
      "The vocabulary files should all contain the same words,\n",
      "just in different formats for compatibility with different libraries.\n",
      "If sizes differ, there might be an issue with the generation process.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the paths from your actual file locations\n",
    "BASE_OUTPUT_DIR = Path(\"../data/vocabulary_dumps/1_word\")\n",
    "SKLEARN_VOCAB_DICT_PATH = BASE_OUTPUT_DIR / \"global_vocabulary_processed_bigram_100_min_df_sklearn_from_sklearn.joblib\"\n",
    "CUML_VOCAB_PARQUET_PATH = BASE_OUTPUT_DIR / \"global_vocabulary_processed_bigram_100_min_df_cuml_from_sklearn.parquet\"\n",
    "CUML_VOCAB_TEXT_PATH = BASE_OUTPUT_DIR / \"global_vocabulary_cuml_from_sklearn.txt\"\n",
    "\n",
    "print(\"Checking vocabulary sizes...\\n\")\n",
    "\n",
    "# Method 1: Check scikit-learn vocabulary dictionary\n",
    "if SKLEARN_VOCAB_DICT_PATH.exists():\n",
    "    try:\n",
    "        sklearn_vocab = joblib.load(SKLEARN_VOCAB_DICT_PATH)\n",
    "        sklearn_size = len(sklearn_vocab)\n",
    "        print(f\"✓ Scikit-learn vocabulary (joblib): {sklearn_size:,} words\")\n",
    "        \n",
    "        # Show a few sample words\n",
    "        sample_words = list(sklearn_vocab.keys())[:10]\n",
    "        print(f\"  Sample words: {sample_words}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading scikit-learn vocabulary: {e}\\n\")\n",
    "else:\n",
    "    print(f\"✗ Scikit-learn vocabulary file not found at: {SKLEARN_VOCAB_DICT_PATH}\\n\")\n",
    "\n",
    "# Method 2: Check cuML vocabulary (Parquet format)\n",
    "if CUML_VOCAB_PARQUET_PATH.exists():\n",
    "    try:\n",
    "        # Try with pandas first (more universally available)\n",
    "        cuml_vocab_df = pd.read_parquet(CUML_VOCAB_PARQUET_PATH)\n",
    "        cuml_parquet_size = len(cuml_vocab_df)\n",
    "        print(f\"✓ cuML vocabulary (Parquet): {cuml_parquet_size:,} words\")\n",
    "        \n",
    "        # Show a few sample words\n",
    "        if 'term' in cuml_vocab_df.columns:\n",
    "            sample_terms = cuml_vocab_df['term'].head(10).tolist()\n",
    "            print(f\"  Sample terms: {sample_terms}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading cuML vocabulary (Parquet): {e}\\n\")\n",
    "else:\n",
    "    print(f\"✗ cuML vocabulary (Parquet) file not found at: {CUML_VOCAB_PARQUET_PATH}\\n\")\n",
    "\n",
    "# Method 3: Check cuML vocabulary (Text format)\n",
    "if CUML_VOCAB_TEXT_PATH.exists():\n",
    "    try:\n",
    "        with open(CUML_VOCAB_TEXT_PATH, 'r', encoding='utf-8') as f:\n",
    "            text_vocab_lines = f.readlines()\n",
    "        cuml_text_size = len([line.strip() for line in text_vocab_lines if line.strip()])\n",
    "        print(f\"✓ cuML vocabulary (Text): {cuml_text_size:,} words\")\n",
    "        \n",
    "        # Show a few sample words\n",
    "        sample_text_words = [line.strip() for line in text_vocab_lines[:10] if line.strip()]\n",
    "        print(f\"  Sample words: {sample_text_words}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading cuML vocabulary (Text): {e}\\n\")\n",
    "else:\n",
    "    print(f\"✗ cuML vocabulary (Text) file not found at: {CUML_VOCAB_TEXT_PATH}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
