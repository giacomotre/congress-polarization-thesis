{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as CumlTfidfVectorizer\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_output_dir = Path(\"data/vocabulary_dumps\") # Base directory for vocabulary files\n",
    "base_dir = Path(\"../data/merged\")\n",
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "base_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cuml_vocab_parquet_path = base_output_dir / \"global_vocabulary_cuml.parquet\"\n",
    "sklearn_vocab_dict_path = base_output_dir / \"global_vocabulary_sklearn.joblib\"\n",
    "\n",
    "all_speeches_list_cpu = []\n",
    "global_min_df = 200\n",
    "\n",
    "print(\"Loading and aggregating speeches from all years (CPU)...\")\n",
    "\n",
    "for i in range(75, 112):\n",
    "    \n",
    "    \"------ Loading files -------\"\n",
    "    print(f'── Processing Congress {i} ──')\n",
    "    year_str = f\"{i:03}\"\n",
    "    house_file = base_dir / f\"house_db/house_merged_{year_str}.csv\"\n",
    "    if base_dir.exists():\n",
    "        try:\n",
    "            df_vocab = pd.read_csv(house_file, usecols=['speech'])\n",
    "            df_vocab.dropna(subset=['speech'], inplace=True)\n",
    "            all_speeches_list_cpu.extend(df_vocab['speech'].astype(str).tolist()) # Ensure speech is string\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {house_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found for vocab generation: {house_file}\")\n",
    "\n",
    "if not all_speeches_list_cpu:\n",
    "    print(\"Error: No speeches collected for vocabulary generation. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Collected {len(all_speeches_list_cpu)} speeches.\")\n",
    "\n",
    "# --- 2. Transfer to GPU (CPU -> GPU) ---\n",
    "print(\"Transferring speeches to GPU...\")\n",
    "try:\n",
    "    all_speeches_cudf_series = cudf.Series(all_speeches_list_cpu)\n",
    "    del all_speeches_list_cpu # Free CPU memory\n",
    "except Exception as e:\n",
    "    print(f\"Error converting list to cudf.Series: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Vocabulary Computation (GPU) using cuML ---\n",
    "print(\"Building global vocabulary with cuML TfidfVectorizer (GPU)...\")\n",
    "try:\n",
    "    vocab_builder_vectorizer_cuml = CumlTfidfVectorizer(\n",
    "        min_df=global_min_df,\n",
    "        ngram_range=(1, 1) # Unigrams\n",
    "        # cuML's TfidfVectorizer doesn't have a direct 'stop_words=None' like sklearn;\n",
    "        # its default behavior is typically no stop word removal unless a list is provided.\n",
    "    )\n",
    "    vocab_builder_vectorizer_cuml.fit(all_speeches_cudf_series)\n",
    "    del all_speeches_cudf_series # Free GPU memory\n",
    "except Exception as e:\n",
    "    print(f\"Error during cuML TfidfVectorizer fitting: {e}\")\n",
    "    exit()\n",
    "\n",
    "# The vocabulary_ attribute in cuML's fitted TfidfVectorizer is a cudf.Series of the terms,\n",
    "# sorted alphabetically (this is typical for cuML's vocabulary handling).\n",
    "gpu_terms_cudf_series = vocab_builder_vectorizer_cuml.vocabulary_\n",
    "print(f\"Generated global vocabulary with {len(gpu_terms_cudf_series)} features on GPU.\")\n",
    "\n",
    "# --- 4. Create and Save Dumps ---\n",
    "\n",
    "# 4a. For cuML: Save the cudf.Series of terms to a Parquet file\n",
    "print(f\"Saving cuML vocabulary (cudf.Series of terms) to {cuml_vocab_parquet_path}...\")\n",
    "try:\n",
    "    # To save a Series to Parquet, it's often easiest to convert it to a DataFrame with one column\n",
    "    gpu_terms_cudf_series.to_frame(name='term').to_parquet(cuml_vocab_parquet_path)\n",
    "    print(f\"Successfully saved cuML vocabulary to {cuml_vocab_parquet_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving cuML vocabulary to Parquet: {e}\")\n",
    "\n",
    "# 4b. For Scikit-learn: Create a dictionary {'term': index} and save with joblib\n",
    "print(f\"Creating and saving scikit-learn vocabulary (dictionary) to {sklearn_vocab_dict_path}...\")\n",
    "try:\n",
    "    # Convert the cuDF Series of terms (on GPU) to a Python list (on CPU)\n",
    "    # The order in gpu_terms_cudf_series determines the indices.\n",
    "    cpu_terms_list = gpu_terms_cudf_series.to_pandas().tolist()\n",
    "    \n",
    "    # Create the scikit-learn style vocabulary dictionary\n",
    "    sklearn_vocab_dict = {term: idx for idx, term in enumerate(cpu_terms_list)}\n",
    "    \n",
    "    joblib.dump(sklearn_vocab_dict, sklearn_vocab_dict_path)\n",
    "    print(f\"Successfully saved scikit-learn vocabulary dictionary to {sklearn_vocab_dict_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating/saving scikit-learn vocabulary dictionary: {e}\")\n",
    "\n",
    "print(\"\\n--- Vocabulary Generation Finished ---\")\n",
    "print(f\"cuML vocabulary (Parquet): {cuml_vocab_parquet_path}\")\n",
    "print(f\"Scikit-learn vocabulary (joblib dict): {sklearn_vocab_dict_path}\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "del vocab_builder_vectorizer_cuml\n",
    "if 'gpu_terms_cudf_series' in locals(): del gpu_terms_cudf_series\n",
    "if 'sklearn_vocab_dict' in locals(): del sklearn_vocab_dict\n",
    "if 'cpu_terms_list' in locals(): del cpu_terms_list\n",
    "\n",
    "cupy.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Data Loading and Initial Aggregation (CPU) ---\n",
    "all_speeches_list_cpu = []\n",
    "congress_years_to_process = [f\"{i:03}\" for i in range(CONGRESS_YEAR_START, CONGRESS_YEAR_END + 1)] # Define these\n",
    "global_min_df = 200 # As per paper example\n",
    "\n",
    "print(\"Loading and aggregating speeches from all years (CPU)...\")\n",
    "for year_str_vocab in congress_years_to_process:\n",
    "    input_csv_path_vocab = Path(f\"data/processed/house_db/house_cleaned_{year_str_vocab}.csv\") # Adjust path\n",
    "    if input_csv_path_vocab.exists():\n",
    "        df_vocab = pd.read_csv(input_csv_path_vocab, usecols=['speech'])\n",
    "        df_vocab.dropna(subset=['speech'], inplace=True)\n",
    "        all_speeches_list_cpu.extend(df_vocab['speech'].tolist())\n",
    "    else:\n",
    "        print(f\"Warning: File not found for vocab generation: {input_csv_path_vocab}\")\n",
    "\n",
    "if not all_speeches_list_cpu:\n",
    "    print(\"Error: No speeches collected for vocabulary generation. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Collected {len(all_speeches_list_cpu)} speeches.\")\n",
    "\n",
    "# --- 2. Transfer to GPU (CPU -> GPU) ---\n",
    "print(\"Transferring speeches to GPU...\")\n",
    "all_speeches_cudf_series = cudf.Series(all_speeches_list_cpu)\n",
    "del all_speeches_list_cpu # Free CPU memory if large\n",
    "\n",
    "# --- 3. Vocabulary Computation (GPU) ---\n",
    "print(\"Building global vocabulary with cuML TfidfVectorizer (GPU)...\")\n",
    "vocab_builder_vectorizer_cuml = CumlTfidfVectorizer(\n",
    "    min_df=global_min_df,\n",
    "    ngram_range=(1, 1)\n",
    "    # cuML's TfidfVectorizer might not have all the same params or defaults as sklearn's\n",
    "    # for stop_words, etc. Check documentation if you need finer control matching sklearn.\n",
    "    # For basic vocabulary building based on min_df and ngrams, this is the core.\n",
    ")\n",
    "vocab_builder_vectorizer_cuml.fit(all_speeches_cudf_series)\n",
    "del all_speeches_cudf_series # Free GPU memory\n",
    "\n",
    "# --- 4. Extracting and Saving Vocabulary (GPU -> CPU) ---\n",
    "# cuML's TfidfVectorizer stores vocabulary in a sorted manner.\n",
    "# The .vocabulary_ attribute in cuML is a cudf.Series of the terms.\n",
    "gpu_vocabulary_terms = vocab_builder_vectorizer_cuml.vocabulary_ # This is a cudf.Series of terms\n",
    "\n",
    "print(f\"Generated global vocabulary with {len(gpu_vocabulary_terms)} features on GPU.\")\n",
    "print(\"Transferring vocabulary to CPU and saving...\")\n",
    "\n",
    "# Convert cuDF Series of terms to a Python list\n",
    "fixed_vocabulary_list_cpu = gpu_vocabulary_terms.to_pandas().tolist()\n",
    "del gpu_vocabulary_terms\n",
    "\n",
    "vocabulary_save_path = Path(\"models/global_fixed_vocabulary_cuml.joblib\") # distinguish if needed\n",
    "vocabulary_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(fixed_vocabulary_list_cpu, vocabulary_save_path)\n",
    "print(f\"Saved global vocabulary list (from cuML) to {vocabulary_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
