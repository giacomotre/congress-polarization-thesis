{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as CumlTfidfVectorizer\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_output_dir = Path(\"data/vocabulary_dumps\") # Base directory for vocabulary files\n",
    "base_dir = Path(\"../data/merged\")\n",
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "base_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cuml_vocab_parquet_path = base_output_dir / \"global_vocabulary_cuml.parquet\"\n",
    "sklearn_vocab_dict_path = base_output_dir / \"global_vocabulary_sklearn.joblib\"\n",
    "\n",
    "all_speeches_list_cpu = []\n",
    "\n",
    "print(\"Loading and aggregating speeches from all years (CPU)...\")\n",
    "\n",
    "for i in range(75, 112):\n",
    "    \n",
    "    \"------ Loading files -------\"\n",
    "    print(f'── Processing Congress {i} ──')\n",
    "    year_str = f\"{i:03}\"\n",
    "    house_file = base_dir / f\"house_db/house_merged_{year_str}.csv\"\n",
    "    if base_dir.exists():\n",
    "        try:\n",
    "            df_vocab = pd.read_csv(house_file, usecols=['speech'])\n",
    "            df_vocab.dropna(subset=['speech'], inplace=True)\n",
    "            all_speeches_list_cpu.extend(df_vocab['speech'].astype(str).tolist()) # Ensure speech is string\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {house_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found for vocab generation: {house_file}\")\n",
    "\n",
    "if not all_speeches_list_cpu:\n",
    "    print(\"Error: No speeches collected for vocabulary generation. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Collected {len(all_speeches_list_cpu)} speeches.\")\n",
    "\n",
    "# --- 2. Transfer to GPU (CPU -> GPU) ---\n",
    "print(\"Transferring speeches to GPU...\")\n",
    "try:\n",
    "    all_speeches_cudf_series = cudf.Series(all_speeches_list_cpu)\n",
    "    del all_speeches_list_cpu # Free CPU memory\n",
    "except Exception as e:\n",
    "    print(f\"Error converting list to cudf.Series: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Vocabulary Computation (GPU) using cuML ---\n",
    "print(\"Building global vocabulary with cuML TfidfVectorizer (GPU)...\")\n",
    "try:\n",
    "    vocab_builder_vectorizer_cuml = CumlTfidfVectorizer(\n",
    "        min_df=200,\n",
    "        ngram_range=(1, 1) # Unigrams\n",
    "        # cuML's TfidfVectorizer doesn't have a direct 'stop_words=None' like sklearn;\n",
    "        # its default behavior is typically no stop word removal unless a list is provided.\n",
    "    )\n",
    "    vocab_builder_vectorizer_cuml.fit(all_speeches_cudf_series)\n",
    "    del all_speeches_cudf_series # Free GPU memory\n",
    "except Exception as e:\n",
    "    print(f\"Error during cuML TfidfVectorizer fitting: {e}\")\n",
    "    exit()\n",
    "\n",
    "# The vocabulary_ attribute in cuML's fitted TfidfVectorizer is a cudf.Series of the terms,\n",
    "# sorted alphabetically (this is typical for cuML's vocabulary handling).\n",
    "gpu_terms_cudf_series = vocab_builder_vectorizer_cuml.vocabulary_\n",
    "print(f\"Generated global vocabulary with {len(gpu_terms_cudf_series)} features on GPU.\")\n",
    "\n",
    "# --- 4. Create and Save Dumps ---\n",
    "\n",
    "# 4a. For cuML: Save the cudf.Series of terms to a Parquet file\n",
    "print(f\"Saving cuML vocabulary (cudf.Series of terms) to {cuml_vocab_parquet_path}...\")\n",
    "try:\n",
    "    # To save a Series to Parquet, it's often easiest to convert it to a DataFrame with one column\n",
    "    gpu_terms_cudf_series.to_frame(name='term').to_parquet(cuml_vocab_parquet_path)\n",
    "    print(f\"Successfully saved cuML vocabulary to {cuml_vocab_parquet_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving cuML vocabulary to Parquet: {e}\")\n",
    "\n",
    "# 4b. For Scikit-learn: Create a dictionary {'term': index} and save with joblib\n",
    "print(f\"Creating and saving scikit-learn vocabulary (dictionary) to {sklearn_vocab_dict_path}...\")\n",
    "try:\n",
    "    # Convert the cuDF Series of terms (on GPU) to a Python list (on CPU)\n",
    "    # The order in gpu_terms_cudf_series determines the indices.\n",
    "    cpu_terms_list = gpu_terms_cudf_series.to_pandas().tolist()\n",
    "    \n",
    "    # Create the scikit-learn style vocabulary dictionary\n",
    "    sklearn_vocab_dict = {term: idx for idx, term in enumerate(cpu_terms_list)}\n",
    "    \n",
    "    joblib.dump(sklearn_vocab_dict, sklearn_vocab_dict_path)\n",
    "    print(f\"Successfully saved scikit-learn vocabulary dictionary to {sklearn_vocab_dict_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating/saving scikit-learn vocabulary dictionary: {e}\")\n",
    "\n",
    "print(\"\\n--- Vocabulary Generation Finished ---\")\n",
    "print(f\"cuML vocabulary (Parquet): {cuml_vocab_parquet_path}\")\n",
    "print(f\"Scikit-learn vocabulary (joblib dict): {sklearn_vocab_dict_path}\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "del vocab_builder_vectorizer_cuml\n",
    "if 'gpu_terms_cudf_series' in locals(): del gpu_terms_cudf_series\n",
    "if 'sklearn_vocab_dict' in locals(): del sklearn_vocab_dict\n",
    "if 'cpu_terms_list' in locals(): del cpu_terms_list\n",
    "\n",
    "cupy.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f25579",
   "metadata": {},
   "source": [
    "# CPU version (actual used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re # Using regex for more flexible searching\n",
    "import joblib # For saving the scikit-learn dictionary\n",
    "\n",
    "# Import scikit-learn's TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as SklearnTfidfVectorizer\n",
    "\n",
    "# cudf is needed only if you want to save the cuML-compatible vocab as Parquet\n",
    "# If you don't have cuDF in the environment where this CPU script runs,\n",
    "# you could save the cpu_terms_list as a simple text file or CSV instead for cuML.\n",
    "# For now, I'll assume cuDF might be available or you'll adapt the cuML saving part.\n",
    "try:\n",
    "    import cudf\n",
    "    CUDF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CUDF_AVAILABLE = False\n",
    "    print(\"Warning: cudf not found. Will not be able to save cuML vocabulary in Parquet format directly from this script.\")\n",
    "    print(\"Consider saving as a text file or CSV instead for the cuML vocabulary list.\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_OUTPUT_DIR = Path(\"/content/drive/MyDrive/congress-polarization-thesis/data/vocabulary_dumps\") # Base directory for vocabulary files\n",
    "BASE_DIR = Path(\"/content/drive/MyDrive/congress-polarization-thesis/data/processed\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define output paths\n",
    "CUML_VOCAB_OUTPUT_PATH = BASE_OUTPUT_DIR / \"global_vocabulary_processed_v2_200_min_df_cuml_from_sklearn.parquet\" # Changed name slightly for clarity\n",
    "SKLEARN_VOCAB_DICT_PATH = BASE_OUTPUT_DIR / \"global_vocabulary_processed_v2_200_min_df_sklearn_from_sklearn.joblib\" # Changed name slightly\n",
    "\n",
    "all_speeches_list_cpu = []\n",
    "GLOBAL_MIN_DF = 200 # Your min_df setting\n",
    "TOKEN_PATTERN_WORDS_ONLY = r\"(?u)\\b[a-zA-Z]{2,}\\b\" # Stricter token pattern\n",
    "\n",
    "print(\"Loading and aggregating speeches from all years (CPU)...\")\n",
    "\n",
    "for i in range(76, 112): # Your full range\n",
    "    print(f'── Processing Congress {i} ──')\n",
    "    year_str = f\"{i:03}\"\n",
    "    house_file = BASE_DIR / f\"house_db/house_cleaned_{year_str}.csv\" # Reading CLEANED files\n",
    "\n",
    "    # Path check was: if base_dir.exists(): which checks the directory, not the file\n",
    "    # Should be: if house_file.exists():\n",
    "    if house_file.exists():\n",
    "        try:\n",
    "            df_vocab = pd.read_csv(house_file, usecols=['speech'])\n",
    "            df_vocab.dropna(subset=['speech'], inplace=True)\n",
    "            all_speeches_list_cpu.extend(df_vocab['speech'].astype(str).tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {house_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found for vocab generation: {house_file}\")\n",
    "\n",
    "if not all_speeches_list_cpu:\n",
    "    print(\"Error: No speeches collected for vocabulary generation. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Collected {len(all_speeches_list_cpu)} speeches.\")\n",
    "\n",
    "#------ Debugging: Inspect Speeches (as before) ------\n",
    "print(\"\\n--- Inspecting First 5 Speeches ---\")\n",
    "for i_debug, speech_debug in enumerate(all_speeches_list_cpu[:5]):\n",
    "    print(f\"Speech {i_debug}: {speech_debug[:200]}...\")\n",
    "    print(f\"  Type: {type(speech_debug)}\")\n",
    "    print(f\"  Length (chars): {len(speech_debug)}\")\n",
    "print(\"--------------------------------\\n\")\n",
    "\n",
    "print(\"\\n--- Checking for long 'words' ---\")\n",
    "long_words = []\n",
    "for speech_debug in all_speeches_list_cpu[:10000]:\n",
    "    if isinstance(speech_debug, str):\n",
    "        words = speech_debug.split() # Simple split for this check\n",
    "        for word in words:\n",
    "            if len(word) > 50:\n",
    "                long_words.append(word)\n",
    "if long_words:\n",
    "    print(f\"Found {len(long_words)} 'words' longer than 50 chars. Examples: {long_words[:10]}\")\n",
    "else:\n",
    "    print(\"No unusually long 'words' found in sample (based on simple split).\")\n",
    "print(\"-----------------------------\\n\")\n",
    "#-------- End Debugging Speeches --------\n",
    "\n",
    "# --- Vocabulary Computation (CPU) using Scikit-learn ---\n",
    "print(\"Building global vocabulary with Scikit-learn TfidfVectorizer (CPU)...\")\n",
    "try:\n",
    "    # Initialize scikit-learn's TfidfVectorizer\n",
    "    # lowercase=False because your SpaCy script should already handle it.\n",
    "    # token_pattern is crucial here if your SpaCy output is space-separated tokens.\n",
    "    # If SpaCy outputs already tokenized text (joined by space), the default tokenizer\n",
    "    # might be okay, but specifying the token_pattern gives more control.\n",
    "    # Since your cleaned text is a string of space-separated lemmas, the default\n",
    "    # tokenizer (which splits on whitespace and punctuation) might still try to\n",
    "    # split your lemmas if they contained internal hyphens not caught by is_alpha.\n",
    "    # Using the stricter token_pattern is safer.\n",
    "    vocab_builder_vectorizer_sklearn = SklearnTfidfVectorizer(\n",
    "        min_df=GLOBAL_MIN_DF,\n",
    "        ngram_range=(1, 1),        # Unigrams\n",
    "        #token_pattern=TOKEN_PATTERN_WORDS_ONLY, # Use the stricter pattern\n",
    "        lowercase=False,           # False, Assuming already lowercased by SpaCy\n",
    "        stop_words=None            # Assuming stop words removed by SpaCy\n",
    "    )\n",
    "\n",
    "    # Fit on the CPU list of speeches\n",
    "    print(f\"Fitting SklearnTfidfVectorizer on {len(all_speeches_list_cpu)} speeches...\")\n",
    "    vocab_builder_vectorizer_sklearn.fit(all_speeches_list_cpu)\n",
    "\n",
    "    # Get the vocabulary dictionary for scikit-learn\n",
    "    sklearn_vocab_dict = vocab_builder_vectorizer_sklearn.vocabulary_\n",
    "\n",
    "    # Get the feature names (terms) in the order of their indices\n",
    "    # This list will be used for the cuML-compatible output\n",
    "    cpu_terms_list = vocab_builder_vectorizer_sklearn.get_feature_names_out().tolist()\n",
    "\n",
    "    print(f\"Generated global vocabulary with {len(cpu_terms_list)} features using scikit-learn.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Scikit-learn TfidfVectorizer fitting: {e}\")\n",
    "    # Consider exiting or handling more gracefully\n",
    "    sklearn_vocab_dict = {}\n",
    "    cpu_terms_list = []\n",
    "    # exit() # Optional: exit if vocab building fails\n",
    "\n",
    "#--- Debugging: Inspect Vocabulary (as before) ---\n",
    "if cpu_terms_list:\n",
    "    print(\"\\n--- Inspecting First 100 Vocabulary Terms (from scikit-learn) ---\")\n",
    "    print(cpu_terms_list[:100])\n",
    "    print(\"-----------------------------------------------------------------\\n\")\n",
    "# --- End Debugging Vocabulary ---\n",
    "\n",
    "\n",
    "# --- Create and Save Dumps ---\n",
    "\n",
    "# 1. For Scikit-learn: Save the dictionary {'term': index}\n",
    "print(f\"Creating and saving scikit-learn vocabulary (dictionary) to {SKLEARN_VOCAB_DICT_PATH}...\")\n",
    "if sklearn_vocab_dict:\n",
    "    try:\n",
    "        joblib.dump(sklearn_vocab_dict, SKLEARN_VOCAB_DICT_PATH)\n",
    "        print(f\"Successfully saved scikit-learn vocabulary dictionary to {SKLEARN_VOCAB_DICT_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating/saving scikit-learn vocabulary dictionary: {e}\")\n",
    "else:\n",
    "    print(\"Scikit-learn vocabulary dictionary is empty, not saving.\")\n",
    "\n",
    "\n",
    "# 2. For cuML: Save the list of terms (cpu_terms_list)\n",
    "#    Option A: As Parquet using cuDF (if available)\n",
    "#    Option B: As a simple text file (one term per line) if cuDF is not available\n",
    "print(f\"Saving cuML-compatible vocabulary (list of terms)...\")\n",
    "if cpu_terms_list:\n",
    "    if CUDF_AVAILABLE:\n",
    "        print(f\"Attempting to save as Parquet to {CUML_VOCAB_OUTPUT_PATH} using cuDF...\")\n",
    "        try:\n",
    "            terms_cudf_series = cudf.Series(cpu_terms_list)\n",
    "            terms_cudf_series.to_frame(name='term').to_parquet(CUML_VOCAB_OUTPUT_PATH)\n",
    "            print(f\"Successfully saved cuML-compatible vocabulary (Parquet) to {CUML_VOCAB_OUTPUT_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cuML-compatible vocabulary to Parquet: {e}\")\n",
    "            print(\"Consider saving as a text file as a fallback if Parquet fails.\")\n",
    "            CUDF_AVAILABLE = False # Fallback to text if save fails\n",
    "\n",
    "    if not CUDF_AVAILABLE: # If cuDF wasn't available or Parquet saving failed\n",
    "        cuml_vocab_text_path = BASE_OUTPUT_DIR / \"global_vocabulary_cuml_from_sklearn.txt\"\n",
    "        print(f\"Saving cuML-compatible vocabulary as text file to {cuml_vocab_text_path}...\")\n",
    "        try:\n",
    "            with open(cuml_vocab_text_path, 'w', encoding='utf-8') as f:\n",
    "                for term in cpu_terms_list:\n",
    "                    f.write(term + \"\\n\")\n",
    "            print(f\"Successfully saved cuML-compatible vocabulary (text file) to {cuml_vocab_text_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cuML-compatible vocabulary to text file: {e}\")\n",
    "else:\n",
    "    print(\"CPU terms list is empty, not saving cuML-compatible vocabulary.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Vocabulary Generation Finished ---\")\n",
    "if sklearn_vocab_dict:\n",
    "    print(f\"Scikit-learn vocabulary (joblib dict): {SKLEARN_VOCAB_DICT_PATH}\")\n",
    "if cpu_terms_list:\n",
    "    if CUDF_AVAILABLE and CUML_VOCAB_OUTPUT_PATH.exists():\n",
    "         print(f\"cuML vocabulary (Parquet from scikit-learn terms): {CUML_VOCAB_OUTPUT_PATH}\")\n",
    "    elif not CUDF_AVAILABLE and (BASE_OUTPUT_DIR / \"global_vocabulary_cuml_from_sklearn.txt\").exists():\n",
    "        print(f\"cuML vocabulary (Text file from scikit-learn terms): {BASE_OUTPUT_DIR / 'global_vocabulary_cuml_from_sklearn.txt'}\")\n",
    "\n",
    "\n",
    "# --- Cleanup (CPU based, so no cupy cleanup needed) ---\n",
    "if 'vocab_builder_vectorizer_sklearn' in locals(): del vocab_builder_vectorizer_sklearn\n",
    "if 'sklearn_vocab_dict' in locals(): del sklearn_vocab_dict\n",
    "if 'cpu_terms_list' in locals(): del cpu_terms_list\n",
    "if 'all_speeches_list_cpu' in locals(): del all_speeches_list_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ed9db",
   "metadata": {},
   "source": [
    "vocabulary dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321736fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data from Joblib file ('../data/vocabulary_dumps/global_vocabulary_processed_v2_200_min_df_sklearn_from_sklearn.joblib'): 15690 features (assuming it's a direct vocabulary list/dict)\n",
      "------------------------------\n",
      "Number of rows (assumed features) from Parquet file ('../data/vocabulary_dumps/global_vocabulary_processed_v2_200_min_df_cuml_from_sklearn.parquet'): 15690\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "VOCABULARY_PATH = \"data/vocabulary_dumps\"\n",
    "\n",
    "# --- Load Vocabulary from Joblib File ---\n",
    "joblib_file_path = '../data/vocabulary_dumps/global_vocabulary_processed_v2_200_min_df_sklearn_from_sklearn.joblib' # Replace with your .joblib file path\n",
    "\n",
    "try:\n",
    "    tfidf_vectorizer_joblib = joblib.load(joblib_file_path)\n",
    "    # Assuming the loaded object is a scikit-learn TF-IDF Vectorizer\n",
    "    # The vocabulary_ attribute is a dictionary where keys are terms and values are feature indices\n",
    "    if hasattr(tfidf_vectorizer_joblib, 'vocabulary_'):\n",
    "        joblib_vocab_len = len(tfidf_vectorizer_joblib.vocabulary_)\n",
    "        print(f\"Length of vocabulary from Joblib file ('{joblib_file_path}'): {joblib_vocab_len} features\")\n",
    "    elif hasattr(tfidf_vectorizer_joblib, 'get_feature_names_out'): # For newer scikit-learn versions\n",
    "        joblib_vocab_len = len(tfidf_vectorizer_joblib.get_feature_names_out())\n",
    "        print(f\"Length of vocabulary from Joblib file ('{joblib_file_path}'): {joblib_vocab_len} features\")\n",
    "    else:\n",
    "        # If it's just a list or dictionary of features already\n",
    "        try:\n",
    "            joblib_vocab_len = len(tfidf_vectorizer_joblib)\n",
    "            print(f\"Length of data from Joblib file ('{joblib_file_path}'): {joblib_vocab_len} features (assuming it's a direct vocabulary list/dict)\")\n",
    "        except TypeError:\n",
    "            print(f\"Error: Could not determine feature count from the object loaded from '{joblib_file_path}'. \"\n",
    "                  \"Please ensure it's a TF-IDF vectorizer or a vocabulary list/dictionary.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Joblib file not found at '{joblib_file_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the Joblib file: {e}\")\n",
    "\n",
    "print(\"-\" * 30) # Separator\n",
    "\n",
    "# --- Load Vocabulary from Parquet File ---\n",
    "parquet_file_path = '../data/vocabulary_dumps/global_vocabulary_processed_v2_200_min_df_cuml_from_sklearn.parquet' # Replace with your .parquet file path\n",
    "\n",
    "try:\n",
    "    # Assuming the Parquet file contains a DataFrame where one column is the vocabulary\n",
    "    # or the DataFrame itself represents the features in some way.\n",
    "    df_parquet = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "    # Option 1: If the Parquet file directly contains a list/series of vocabulary terms\n",
    "    # For example, a DataFrame with a single column named 'vocabulary' or 'features'\n",
    "    if 'vocabulary' in df_parquet.columns and len(df_parquet.columns) == 1:\n",
    "        parquet_vocab_len = len(df_parquet['vocabulary'])\n",
    "        print(f\"Length of vocabulary from Parquet file ('{parquet_file_path}', column 'vocabulary'): {parquet_vocab_len} features\")\n",
    "    elif 'feature' in df_parquet.columns and len(df_parquet.columns) == 1:\n",
    "        parquet_vocab_len = len(df_parquet['feature'])\n",
    "        print(f\"Length of vocabulary from Parquet file ('{parquet_file_path}', column 'feature'): {parquet_vocab_len} features\")\n",
    "    # Option 2: If the Parquet file represents features where each row or column is a feature\n",
    "    # This depends heavily on how you saved it.\n",
    "    # If each row is a feature:\n",
    "    # parquet_vocab_len = len(df_parquet)\n",
    "    # If each column is a feature (less common for just vocabulary):\n",
    "    # parquet_vocab_len = len(df_parquet.columns)\n",
    "    # For this example, let's assume it's a DataFrame where the number of rows is the number of features.\n",
    "    else:\n",
    "        parquet_vocab_len = len(df_parquet) # Or use df_parquet.shape[0]\n",
    "        print(f\"Number of rows (assumed features) from Parquet file ('{parquet_file_path}'): {parquet_vocab_len}\")\n",
    "        # If your features are columns, you might want:\n",
    "        # print(f\"Number of columns from Parquet file ('{parquet_file_path}'): {df_parquet.shape[1]}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Parquet file not found at '{parquet_file_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the Parquet file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
