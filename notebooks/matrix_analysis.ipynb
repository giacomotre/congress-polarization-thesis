{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ba70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political Text Classification Bias Analysis\n",
    "# A comprehensive notebook for analyzing bias in Republican vs Democrat classification models\n",
    "\n",
    "# ## Setup and Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(\"üéØ Ready to analyze classification bias in political text data\")\n",
    "\n",
    "# ## Configuration\n",
    "\n",
    "# File configuration\n",
    "CSV_FILE = \"tfidf_lr_performance_avg.csv\"  # Your CSV file path\n",
    "PARTY_MAP = {'D': 0, 'R': 1}  # Adjust if your mapping is different\n",
    "OUTPUT_DIR = \"lr_bias_analysis\"  # Output directory for results\n",
    "\n",
    "# Analysis parameters\n",
    "SIGNIFICANCE_LEVEL = 0.05  # Alpha level for statistical tests\n",
    "FIGURE_SIZE = (20, 15)  # Size for comprehensive plots\n",
    "\n",
    "print(f\"üìÅ Input file: {CSV_FILE}\")\n",
    "print(f\"üèõÔ∏è Party mapping: {PARTY_MAP}\")\n",
    "print(f\"üìä Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# ## Data Loading and Preprocessing\n",
    "\n",
    "def load_and_preprocess_data(csv_path, party_map):\n",
    "    \"\"\"\n",
    "    Load and preprocess the performance data for bias analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Loading data from {csv_path}...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úÖ Loaded {len(df)} records\")\n",
    "        print(f\"üìã Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Ensure we have required confusion matrix columns\n",
    "        required_cols = ['tn', 'fp', 'fn', 'tp']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Handle potential NaN or zero values\n",
    "        for col in required_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        \n",
    "        # Create reverse party map for labeling\n",
    "        reverse_party_map = {v: k for k, v in party_map.items()}\n",
    "        \n",
    "        # Calculate bias metrics\n",
    "        eps = 1e-10  # Small epsilon to avoid division by zero\n",
    "        \n",
    "        # Precision: TP / (TP + FP) for class 1, TN / (TN + FN) for class 0\n",
    "        df['precision_class_0'] = df['tn'] / (df['tn'] + df['fn'] + eps)\n",
    "        df['precision_class_1'] = df['tp'] / (df['tp'] + df['fp'] + eps)\n",
    "        \n",
    "        # Recall: TP / (TP + FN) for class 1, TN / (TN + FP) for class 0\n",
    "        df['recall_class_0'] = df['tn'] / (df['tn'] + df['fp'] + eps)\n",
    "        df['recall_class_1'] = df['tp'] / (df['tp'] + df['fn'] + eps)\n",
    "        \n",
    "        # Bias metrics\n",
    "        df['precision_difference'] = df['precision_class_1'] - df['precision_class_0']\n",
    "        df['recall_difference'] = df['recall_class_1'] - df['recall_class_0']\n",
    "        \n",
    "        # Balanced accuracy\n",
    "        df['balanced_accuracy'] = (df['recall_class_0'] + df['recall_class_1']) / 2\n",
    "        \n",
    "        # Class distributions\n",
    "        df['total_predictions'] = df['tp'] + df['fp'] + df['tn'] + df['fn']\n",
    "        df['predicted_class_0'] = df['tn'] + df['fn']\n",
    "        df['predicted_class_1'] = df['tp'] + df['fp']\n",
    "        df['actual_class_0'] = df['tn'] + df['fp']\n",
    "        df['actual_class_1'] = df['tp'] + df['fn']\n",
    "        \n",
    "        # Prediction bias\n",
    "        df['predicted_rate_class_1'] = df['predicted_class_1'] / (df['total_predictions'] + eps)\n",
    "        df['actual_rate_class_1'] = df['actual_class_1'] / (df['total_predictions'] + eps)\n",
    "        df['prediction_bias'] = df['predicted_rate_class_1'] - df['actual_rate_class_1']\n",
    "        \n",
    "        # Error rates\n",
    "        df['false_positive_rate'] = df['fp'] / (df['fp'] + df['tn'] + eps)\n",
    "        df['false_negative_rate'] = df['fn'] / (df['fn'] + df['tp'] + eps)\n",
    "        df['fpr_fnr_difference'] = df['false_positive_rate'] - df['false_negative_rate']\n",
    "        \n",
    "        print(f\"‚úÖ Successfully calculated bias metrics\")\n",
    "        return df, reverse_party_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the data\n",
    "df, reverse_party_map = load_and_preprocess_data(CSV_FILE, PARTY_MAP)\n",
    "\n",
    "# ## Data Overview\n",
    "\n",
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìà Total records: {len(df)}\")\n",
    "print(f\"üóìÔ∏è Congress years: {sorted(df['year'].unique())}\")\n",
    "print(f\"üéØ Mean accuracy: {df['accuracy'].mean():.4f} ¬± {df['accuracy'].std():.4f}\")\n",
    "print(f\"üéØ Mean F1 score: {df['f1_score'].mean():.4f} ¬± {df['f1_score'].std():.4f}\")\n",
    "print(f\"üéØ Mean AUC: {df['auc'].mean():.4f} ¬± {df['auc'].std():.4f}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nüìã CONFUSION MATRIX STATISTICS\")\n",
    "print(\"-\" * 30)\n",
    "cm_stats = df[['tn', 'fp', 'fn', 'tp']].describe()\n",
    "print(cm_stats)\n",
    "\n",
    "# ## Statistical Bias Analysis\n",
    "\n",
    "def calculate_bias_statistics(df, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive bias statistics with significance tests\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Mean bias metrics\n",
    "    results['mean_precision_difference'] = df['precision_difference'].mean()\n",
    "    results['mean_recall_difference'] = df['recall_difference'].mean()\n",
    "    results['mean_prediction_bias'] = df['prediction_bias'].mean()\n",
    "    results['mean_fpr_fnr_difference'] = df['fpr_fnr_difference'].mean()\n",
    "    \n",
    "    # Standard deviations\n",
    "    results['std_precision_difference'] = df['precision_difference'].std()\n",
    "    results['std_recall_difference'] = df['recall_difference'].std()\n",
    "    results['std_prediction_bias'] = df['prediction_bias'].std()\n",
    "    \n",
    "    # Statistical significance tests (one-sample t-tests against 0)\n",
    "    _, results['precision_bias_p_value'] = stats.ttest_1samp(df['precision_difference'], 0)\n",
    "    _, results['recall_bias_p_value'] = stats.ttest_1samp(df['recall_difference'], 0)\n",
    "    _, results['prediction_bias_p_value'] = stats.ttest_1samp(df['prediction_bias'], 0)\n",
    "    _, results['fpr_fnr_bias_p_value'] = stats.ttest_1samp(df['fpr_fnr_difference'], 0)\n",
    "    \n",
    "    # Temporal consistency\n",
    "    year_bias = df.groupby('year').agg({\n",
    "        'precision_difference': 'mean',\n",
    "        'recall_difference': 'mean',\n",
    "        'prediction_bias': 'mean'\n",
    "    })\n",
    "    \n",
    "    results['temporal_consistency'] = {\n",
    "        'precision_std_across_years': year_bias['precision_difference'].std(),\n",
    "        'recall_std_across_years': year_bias['recall_difference'].std(),\n",
    "        'prediction_std_across_years': year_bias['prediction_bias'].std()\n",
    "    }\n",
    "    \n",
    "    # Significance flags\n",
    "    results['significant_precision_bias'] = results['precision_bias_p_value'] < significance_level\n",
    "    results['significant_recall_bias'] = results['recall_bias_p_value'] < significance_level\n",
    "    results['significant_prediction_bias'] = results['prediction_bias_p_value'] < significance_level\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate bias statistics\n",
    "bias_stats = calculate_bias_statistics(df, SIGNIFICANCE_LEVEL)\n",
    "\n",
    "print(\"üîç BIAS ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Precision Bias ({reverse_party_map[1]} - {reverse_party_map[0]}): {bias_stats['mean_precision_difference']:.4f} ¬± {bias_stats['std_precision_difference']:.4f}\")\n",
    "print(f\"üìä Recall Bias ({reverse_party_map[1]} - {reverse_party_map[0]}): {bias_stats['mean_recall_difference']:.4f} ¬± {bias_stats['std_recall_difference']:.4f}\")\n",
    "print(f\"üìä Prediction Bias: {bias_stats['mean_prediction_bias']:.4f} ¬± {bias_stats['std_prediction_bias']:.4f}\")\n",
    "\n",
    "print(\"\\nüß™ STATISTICAL SIGNIFICANCE (p-values)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Precision Bias: {bias_stats['precision_bias_p_value']:.6f} {'‚úÖ Significant' if bias_stats['significant_precision_bias'] else '‚ùå Not significant'}\")\n",
    "print(f\"Recall Bias: {bias_stats['recall_bias_p_value']:.6f} {'‚úÖ Significant' if bias_stats['significant_recall_bias'] else '‚ùå Not significant'}\")\n",
    "print(f\"Prediction Bias: {bias_stats['prediction_bias_p_value']:.6f} {'‚úÖ Significant' if bias_stats['significant_prediction_bias'] else '‚ùå Not significant'}\")\n",
    "\n",
    "# ## Comprehensive Visualization\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{OUTPUT_DIR}/plots\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def create_comprehensive_bias_plots(df, reverse_party_map, output_dir):\n",
    "    \"\"\"\n",
    "    Create comprehensive bias analysis visualizations\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=FIGURE_SIZE)\n",
    "    \n",
    "    # 1. Per-Class Performance Over Time\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    years = sorted(df['year'].unique())\n",
    "    year_metrics = df.groupby('year').agg({\n",
    "        'precision_class_0': 'mean',\n",
    "        'precision_class_1': 'mean',\n",
    "        'recall_class_0': 'mean',\n",
    "        'recall_class_1': 'mean'\n",
    "    })\n",
    "    \n",
    "    ax1.plot(years, year_metrics['precision_class_0'], 'b-o', label=f'{reverse_party_map[0]} Precision', alpha=0.8, linewidth=2)\n",
    "    ax1.plot(years, year_metrics['precision_class_1'], 'r-o', label=f'{reverse_party_map[1]} Precision', alpha=0.8, linewidth=2)\n",
    "    ax1.plot(years, year_metrics['recall_class_0'], 'b--s', label=f'{reverse_party_map[0]} Recall', alpha=0.8, linewidth=2)\n",
    "    ax1.plot(years, year_metrics['recall_class_1'], 'r--s', label=f'{reverse_party_map[1]} Recall', alpha=0.8, linewidth=2)\n",
    "    ax1.set_title('Per-Class Performance Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Congress Year')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Precision Bias Distribution\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    ax2.hist(df['precision_difference'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.axvline(0, color='red', linestyle='--', linewidth=2, label='No Bias')\n",
    "    ax2.axvline(df['precision_difference'].mean(), color='orange', linewidth=3, label='Mean Bias')\n",
    "    ax2.set_title(f'Precision Bias Distribution\\n(Mean: {df[\"precision_difference\"].mean():.4f})', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel(f'Precision Difference ({reverse_party_map[1]} - {reverse_party_map[0]})')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Recall Bias Distribution\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    ax3.hist(df['recall_difference'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    ax3.axvline(0, color='red', linestyle='--', linewidth=2, label='No Bias')\n",
    "    ax3.axvline(df['recall_difference'].mean(), color='orange', linewidth=3, label='Mean Bias')\n",
    "    ax3.set_title(f'Recall Bias Distribution\\n(Mean: {df[\"recall_difference\"].mean():.4f})', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel(f'Recall Difference ({reverse_party_map[1]} - {reverse_party_map[0]})')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Prediction Bias Over Time\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    year_pred_bias = df.groupby('year')['prediction_bias'].mean()\n",
    "    ax4.plot(years, year_pred_bias, 'g-o', linewidth=3, markersize=8, alpha=0.8)\n",
    "    ax4.axhline(0, color='red', linestyle='--', linewidth=2, label='No Bias')\n",
    "    ax4.fill_between(years, year_pred_bias, 0, alpha=0.3, color='green')\n",
    "    ax4.set_title('Prediction Bias Over Time', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Congress Year')\n",
    "    ax4.set_ylabel(f'Prediction Bias (toward {reverse_party_map[1]})')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Average Confusion Matrix Heatmap\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    avg_cm = np.array([[df['tn'].mean(), df['fp'].mean()],\n",
    "                      [df['fn'].mean(), df['tp'].mean()]])\n",
    "    sns.heatmap(avg_cm, annot=True, fmt='.1f', \n",
    "               xticklabels=[f'Pred {reverse_party_map[0]}', f'Pred {reverse_party_map[1]}'],\n",
    "               yticklabels=[f'True {reverse_party_map[0]}', f'True {reverse_party_map[1]}'],\n",
    "               cmap='Blues', ax=ax5, cbar_kws={'label': 'Average Count'})\n",
    "    ax5.set_title('Average Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 6. Balanced vs Regular Accuracy\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    ax6.scatter(df['accuracy'], df['balanced_accuracy'], alpha=0.6, s=50, c='purple')\n",
    "    min_acc = min(df['accuracy'].min(), df['balanced_accuracy'].min())\n",
    "    max_acc = max(df['accuracy'].max(), df['balanced_accuracy'].max())\n",
    "    ax6.plot([min_acc, max_acc], [min_acc, max_acc], 'r--', alpha=0.8, linewidth=2, label='Perfect Agreement')\n",
    "    ax6.set_xlabel('Regular Accuracy')\n",
    "    ax6.set_ylabel('Balanced Accuracy')\n",
    "    ax6.set_title('Balanced vs Regular Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Bias vs Performance Correlation\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    ax7.scatter(df['accuracy'], df['precision_difference'], alpha=0.6, s=50, c='purple')\n",
    "    ax7.set_xlabel('Overall Accuracy')\n",
    "    ax7.set_ylabel(f'Precision Bias ({reverse_party_map[1]} - {reverse_party_map[0]})')\n",
    "    ax7.set_title('Accuracy vs Precision Bias', fontsize=14, fontweight='bold')\n",
    "    ax7.axhline(0, color='red', linestyle='--', alpha=0.8)\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. Class Distribution in Data\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    class_0_ratio = df['actual_class_0'] / df['total_predictions']\n",
    "    class_1_ratio = df['actual_class_1'] / df['total_predictions']\n",
    "    \n",
    "    ax8.hist([class_0_ratio, class_1_ratio], bins=15, alpha=0.7, \n",
    "            label=[f'{reverse_party_map[0]} in Data', f'{reverse_party_map[1]} in Data'],\n",
    "            color=['blue', 'red'])\n",
    "    ax8.set_title('Class Distribution in Test Data', fontsize=14, fontweight='bold')\n",
    "    ax8.set_xlabel('Proportion')\n",
    "    ax8.set_ylabel('Frequency')\n",
    "    ax8.legend()\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Error Rates by Class Over Time\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    year_fpr = df.groupby('year')['false_positive_rate'].mean()\n",
    "    year_fnr = df.groupby('year')['false_negative_rate'].mean()\n",
    "    \n",
    "    ax9.plot(years, year_fpr, 'b-o', label=f'{reverse_party_map[0]} Error Rate (FPR)', alpha=0.8, linewidth=2)\n",
    "    ax9.plot(years, year_fnr, 'r-o', label=f'{reverse_party_map[1]} Error Rate (FNR)', alpha=0.8, linewidth=2)\n",
    "    ax9.set_title('Error Rates by Class Over Time', fontsize=14, fontweight='bold')\n",
    "    ax9.set_xlabel('Congress Year')\n",
    "    ax9.set_ylabel('Error Rate')\n",
    "    ax9.legend()\n",
    "    ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = f\"{output_dir}/plots/comprehensive_bias_analysis.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"üíæ Comprehensive plot saved to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create the comprehensive visualization\n",
    "print(\"üé® Creating comprehensive bias visualization...\")\n",
    "fig = create_comprehensive_bias_plots(df, reverse_party_map, OUTPUT_DIR)\n",
    "\n",
    "# ## Detailed Bias Report\n",
    "\n",
    "def generate_bias_report(df, bias_stats, reverse_party_map, output_path):\n",
    "    \"\"\"\n",
    "    Generate a detailed text report of the bias analysis\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\" * 70)\n",
    "    report.append(\"POLITICAL TEXT CLASSIFICATION BIAS ANALYSIS REPORT\")\n",
    "    report.append(\"=\" * 70)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Dataset overview\n",
    "    report.append(\"DATASET OVERVIEW:\")\n",
    "    report.append(\"-\" * 20)\n",
    "    report.append(f\"Total records analyzed: {len(df)}\")\n",
    "    report.append(f\"Congress years covered: {sorted(df['year'].unique())}\")\n",
    "    report.append(f\"Mean accuracy: {df['accuracy'].mean():.4f} ¬± {df['accuracy'].std():.4f}\")\n",
    "    report.append(f\"Mean F1 score: {df['f1_score'].mean():.4f} ¬± {df['f1_score'].std():.4f}\")\n",
    "    report.append(f\"Mean AUC: {df['auc'].mean():.4f} ¬± {df['auc'].std():.4f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Bias metrics\n",
    "    report.append(\"BIAS METRICS:\")\n",
    "    report.append(\"-\" * 15)\n",
    "    report.append(f\"Precision Bias ({reverse_party_map[1]} - {reverse_party_map[0]}): {bias_stats['mean_precision_difference']:.4f} ¬± {bias_stats['std_precision_difference']:.4f}\")\n",
    "    report.append(f\"Recall Bias ({reverse_party_map[1]} - {reverse_party_map[0]}): {bias_stats['mean_recall_difference']:.4f} ¬± {bias_stats['std_recall_difference']:.4f}\")\n",
    "    report.append(f\"Prediction Bias: {bias_stats['mean_prediction_bias']:.4f} ¬± {bias_stats['std_prediction_bias']:.4f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    report.append(\"STATISTICAL SIGNIFICANCE (p-values):\")\n",
    "    report.append(\"-\" * 40)\n",
    "    report.append(f\"Precision Bias: {bias_stats['precision_bias_p_value']:.6f}\")\n",
    "    report.append(f\"Recall Bias: {bias_stats['recall_bias_p_value']:.6f}\")\n",
    "    report.append(f\"Prediction Bias: {bias_stats['prediction_bias_p_value']:.6f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Interpretation\n",
    "    report.append(\"INTERPRETATION:\")\n",
    "    report.append(\"-\" * 15)\n",
    "    \n",
    "    alpha = SIGNIFICANCE_LEVEL\n",
    "    if bias_stats['significant_precision_bias']:\n",
    "        direction = reverse_party_map[1] if bias_stats['mean_precision_difference'] > 0 else reverse_party_map[0]\n",
    "        magnitude = \"favoring\" if bias_stats['mean_precision_difference'] > 0 else \"disfavoring\"\n",
    "        report.append(f\"‚Ä¢ SIGNIFICANT precision bias detected {magnitude} {direction}\")\n",
    "    else:\n",
    "        report.append(\"‚Ä¢ No statistically significant precision bias detected\")\n",
    "        \n",
    "    if bias_stats['significant_recall_bias']:\n",
    "        direction = reverse_party_map[1] if bias_stats['mean_recall_difference'] > 0 else reverse_party_map[0]\n",
    "        quality = \"better\" if bias_stats['mean_recall_difference'] > 0 else \"worse\"\n",
    "        report.append(f\"‚Ä¢ SIGNIFICANT recall bias: model is {quality} at identifying {direction}\")\n",
    "    else:\n",
    "        report.append(\"‚Ä¢ No statistically significant recall bias detected\")\n",
    "        \n",
    "    if bias_stats['significant_prediction_bias']:\n",
    "        direction = reverse_party_map[1] if bias_stats['mean_prediction_bias'] > 0 else reverse_party_map[0]\n",
    "        tendency = \"over-predicts\" if bias_stats['mean_prediction_bias'] > 0 else \"under-predicts\"\n",
    "        report.append(f\"‚Ä¢ SIGNIFICANT prediction bias: model {tendency} {direction}\")\n",
    "    else:\n",
    "        report.append(\"‚Ä¢ No statistically significant prediction bias detected\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Temporal consistency\n",
    "    report.append(\"TEMPORAL CONSISTENCY:\")\n",
    "    report.append(\"-\" * 20)\n",
    "    temporal = bias_stats['temporal_consistency']\n",
    "    report.append(f\"Precision bias std across years: {temporal['precision_std_across_years']:.4f}\")\n",
    "    report.append(f\"Recall bias std across years: {temporal['recall_std_across_years']:.4f}\")\n",
    "    report.append(f\"Prediction bias std across years: {temporal['prediction_std_across_years']:.4f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"RECOMMENDATIONS:\")\n",
    "    report.append(\"-\" * 15)\n",
    "    \n",
    "    if any([bias_stats['significant_precision_bias'], \n",
    "            bias_stats['significant_recall_bias'],\n",
    "            bias_stats['significant_prediction_bias']]):\n",
    "        report.append(\"‚Ä¢ BIAS DETECTED - Consider implementing mitigation strategies:\")\n",
    "        report.append(\"  - Use balanced class weights during training\")\n",
    "        report.append(\"  - Optimize classification thresholds separately for each class\")\n",
    "        report.append(\"  - Apply post-processing calibration techniques\")\n",
    "        report.append(\"  - Consider adversarial debiasing methods\")\n",
    "        report.append(\"  - Increase training data for underrepresented scenarios\")\n",
    "    else:\n",
    "        report.append(\"‚Ä¢ Model appears relatively unbiased\")\n",
    "        report.append(\"‚Ä¢ Continue monitoring bias metrics in future evaluations\")\n",
    "        report.append(\"‚Ä¢ Consider testing on additional validation sets\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"=\" * 70)\n",
    "    \n",
    "    report_text = \"\\n\".join(report)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(f\"üìÑ Detailed report saved to: {output_path}\")\n",
    "    \n",
    "    return report_text\n",
    "\n",
    "# Generate the detailed report\n",
    "report_path = f\"{OUTPUT_DIR}/bias_analysis_report.txt\"\n",
    "report_text = generate_bias_report(df, bias_stats, reverse_party_map, report_path)\n",
    "\n",
    "# Display the report in the notebook\n",
    "print(\"üìã DETAILED BIAS ANALYSIS REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(report_text)\n",
    "\n",
    "# ## Summary and Next Steps\n",
    "\n",
    "print(\"\\n\" + \"üéØ\" * 25)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"üéØ\" * 25)\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {OUTPUT_DIR}/\")\n",
    "print(f\"   üìä Plots: {OUTPUT_DIR}/plots/\")\n",
    "print(f\"   üìÑ Report: {OUTPUT_DIR}/bias_analysis_report.txt\")\n",
    "\n",
    "print(\"\\nüîç KEY FINDINGS:\")\n",
    "if bias_stats['significant_precision_bias']:\n",
    "    direction = \"Republicans\" if bias_stats['mean_precision_difference'] > 0 else \"Democrats\"\n",
    "    print(f\"   ‚ö†Ô∏è  Significant precision bias favoring {direction}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No significant precision bias\")\n",
    "\n",
    "if bias_stats['significant_recall_bias']:\n",
    "    direction = \"Republicans\" if bias_stats['mean_recall_difference'] > 0 else \"Democrats\"\n",
    "    print(f\"   ‚ö†Ô∏è  Significant recall bias favoring {direction}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No significant recall bias\")\n",
    "\n",
    "if bias_stats['significant_prediction_bias']:\n",
    "    direction = \"Republicans\" if bias_stats['mean_prediction_bias'] > 0 else \"Democrats\"\n",
    "    print(f\"   ‚ö†Ô∏è  Significant prediction bias toward {direction}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No significant prediction bias\")\n",
    "\n",
    "print(\"\\nüí° NEXT STEPS:\")\n",
    "if any([bias_stats['significant_precision_bias'], \n",
    "        bias_stats['significant_recall_bias'],\n",
    "        bias_stats['significant_prediction_bias']]):\n",
    "    print(\"   üîß Implement bias mitigation techniques\")\n",
    "    print(\"   üìä Re-evaluate with balanced class weights\")\n",
    "    print(\"   üéØ Optimize classification thresholds\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Model appears unbiased - continue monitoring\")\n",
    "    print(\"   üìà Consider testing on additional datasets\")\n",
    "\n",
    "print(\"\\nüöÄ Happy analyzing!\")\n",
    "\n",
    "# ## Optional: Interactive Exploration\n",
    "\n",
    "# Uncomment the following section if you want to explore the data interactively\n",
    "\n",
    "\"\"\"\n",
    "# Interactive data exploration\n",
    "print(\"\\\\nüîç INTERACTIVE EXPLORATION\")\n",
    "print(\"=\" * 30)\n",
    "print(\"The following variables are available for further analysis:\")\n",
    "print(\"- df: Main dataframe with all bias metrics\")\n",
    "print(\"- bias_stats: Dictionary with calculated bias statistics\")\n",
    "print(\"- reverse_party_map: Party label mapping\")\n",
    "\n",
    "# Example: Examine specific years with high bias\n",
    "high_bias_years = df[abs(df['precision_difference']) > 0.1]['year'].unique()\n",
    "if len(high_bias_years) > 0:\n",
    "    print(f\"\\\\nüìÖ Years with high precision bias (>0.1): {sorted(high_bias_years)}\")\n",
    "    \n",
    "# Example: Correlation analysis\n",
    "correlation = df['accuracy'].corr(df['precision_difference'])\n",
    "print(f\"\\\\nüìà Correlation between accuracy and precision bias: {correlation:.4f}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
