{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae881eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import openai\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "import numpy as np # Make sure numpy is imported for potential use\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ffd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to your congress files\n",
    "data_path = \"../data/processed/house_db/\"\n",
    "\n",
    "# Define congress range (76 to 111 inclusive)\n",
    "congress_start = 76\n",
    "congress_end = 111\n",
    "\n",
    "print(f\"Loading congress files from {congress_start} to {congress_end}\")\n",
    "\n",
    "# Initialize empty list to store all dataframes\n",
    "all_datasets = []\n",
    "\n",
    "# Loop through each congress number\n",
    "for congress_num in range(congress_start, congress_end + 1):\n",
    "    try:\n",
    "        # Format congress number with leading zeros (e.g., 076, 077, etc.)\n",
    "        congress_str = f\"{congress_num:03d}\"\n",
    "\n",
    "        # Construct file path\n",
    "        file_path = f\"{data_path}house_cleaned_{congress_str}.csv\"\n",
    "\n",
    "        # Load the dataset\n",
    "        dataset = pd.read_csv(file_path)\n",
    "\n",
    "        # Add congress number as a column for reference\n",
    "        dataset['congress_num'] = congress_str\n",
    "\n",
    "        # Select only the columns we need\n",
    "        dataset_subset = dataset[[\"speech\", \"party\", \"speech_id\", \"congress_num\"]].copy()\n",
    "\n",
    "        # Add to our list\n",
    "        all_datasets.append(dataset_subset)\n",
    "\n",
    "        print(f\"Loaded Congress {congress_str}: {len(dataset_subset)} speeches\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: house_cleaned_{congress_str}.csv - skipping\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Congress {congress_str}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Combine all datasets into one\n",
    "if all_datasets:\n",
    "    combined_dataset = pd.concat(all_datasets, ignore_index=True)\n",
    "\n",
    "    print(f\"\\nCombined dataset created!\")\n",
    "    print(f\"Total speeches: {len(combined_dataset)}\")\n",
    "    print(f\"Columns: {list(combined_dataset.columns)}\")\n",
    "    print(f\"Congress numbers included: {sorted(combined_dataset['congress_num'].unique())}\")\n",
    "\n",
    "    # Extract the individual arrays for your analysis\n",
    "    speeches = combined_dataset[\"speech\"]\n",
    "    parties = combined_dataset[\"party\"]\n",
    "    speech_id = combined_dataset[\"speech_id\"]\n",
    "    congress_nums = combined_dataset[\"congress_num\"]\n",
    "\n",
    "    print(f\"\\nData ready for analysis:\")\n",
    "    print(f\"  - speeches: {len(speeches)} entries\")\n",
    "    print(f\"  - parties: {len(parties)} entries\")\n",
    "    print(f\"  - speech_id: {len(speech_id)} entries\")\n",
    "    print(f\"  - congress_nums: {len(congress_nums)} entries\")\n",
    "\n",
    "else:\n",
    "    print(\"No datasets were successfully loaded!\")\n",
    "    combined_dataset = None\n",
    "\n",
    "# Step 1: Filter speeches, parties, AND speech_ids to only include valid string entries\n",
    "valid_indices = []\n",
    "filtered_speeches = []\n",
    "filtered_parties = []\n",
    "filtered_speech_ids = []  # Add this\n",
    "\n",
    "for idx, speech in enumerate(speeches):\n",
    "    if isinstance(speech, str) and speech.strip():\n",
    "        valid_indices.append(idx)\n",
    "        filtered_speeches.append(speech)\n",
    "        filtered_parties.append(parties[idx])\n",
    "        filtered_speech_ids.append(speech_id[idx])  # Add this line\n",
    "    else:\n",
    "        print(f\"Skipping non-string or empty entry at index {idx}\")\n",
    "\n",
    "# Step 2: Create sentence mapping using original speech_ids\n",
    "sentence_mapping = []\n",
    "sentence_counter = 0\n",
    "\n",
    "for speech_idx, speech in enumerate(filtered_speeches):\n",
    "    original_speech_id = filtered_speech_ids[speech_idx]  # Get the original speech_id\n",
    "    sentences_in_speech = sent_tokenize(speech)\n",
    "    for sent in sentences_in_speech:\n",
    "        sentence_mapping.append((sentence_counter, original_speech_id))  # Use original speech_id\n",
    "        sentence_counter += 1\n",
    "\n",
    "# Flatten the list of sentences from filtered speeches\n",
    "sentences = [sentence for speech in filtered_speeches for sentence in sent_tokenize(speech)]\n",
    "\n",
    "print(f\"Total sentences created: {len(sentences)}\")\n",
    "print(f\"Sentence mapping length: {len(sentence_mapping)}\")\n",
    "\n",
    "# Assuming 'sentences' variable is already a list of strings from your previous data aggregation step\n",
    "print(f\"Type of 'sentences': {type(sentences)}\")\n",
    "# Expected output: Type of 'sentences': <class 'list'>\n",
    "\n",
    "# 1. Check for and use GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Load the model and move it to the appropriate device\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# 3. Enable float16 precision if on GPU for further speedup and memory efficiency\n",
    "if device == 'cuda':\n",
    "    embedding_model.half() # Converts model parameters to float16\n",
    "\n",
    "# 4. Pre-calculate embeddings with an optimized batch size\n",
    "# Experiment with batch_size (e.g., 128, 256, 512, 1024) based on your GPU's VRAM.\n",
    "# Start higher and reduce if you hit memory errors.\n",
    "print(\"Calculating embeddings...\")\n",
    "embeddings = embedding_model.encode(\n",
    "    sentences, # This is already your list of strings\n",
    "    show_progress_bar=True,\n",
    "    batch_size=512, # Adjust this value as needed\n",
    "    convert_to_tensor=True # Keep embeddings on GPU as a PyTorch tensor for potential follow-up GPU operations\n",
    ")\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embeddings type: {type(embeddings)}\") # Should be torch.Tensor if convert_to_tensor=True\n",
    "\n",
    "\n",
    "# Optional: If your next steps specifically require a NumPy array on CPU:\n",
    "embeddings_numpy = embeddings.cpu().numpy()\n",
    "print(f\"Embeddings NumPy shape: {embeddings_numpy.shape}\")\n",
    "print(f\"Embeddings NumPy type: {type(embeddings_numpy)}\")\n",
    "\n",
    "# New PCA model:\n",
    "pca_model = PCA(n_components=40, random_state=42)\n",
    "\n",
    "# Use bigger sample for tuning\n",
    "sample_size = 200000\n",
    "sample_idx = np.random.choice(len(embeddings_numpy), sample_size, replace=False)\n",
    "sample_embeddings = embeddings_numpy[sample_idx]\n",
    "\n",
    "# Test different PCA components and k values\n",
    "test_components = [20, 30, 40, 50]\n",
    "test_ks = [20, 25, 30, 35]\n",
    "\n",
    "best_overall_score = -1\n",
    "best_components = 100\n",
    "best_k = 40\n",
    "\n",
    "print(\"Testing different PCA components and k values...\")\n",
    "results = []\n",
    "\n",
    "for n_comp in test_components:\n",
    "    print(f\"\\nTesting n_components = {n_comp}\")\n",
    "\n",
    "    # Apply PCA with current component count\n",
    "    pca_temp = PCA(n_components=n_comp, random_state=42)\n",
    "    pca_sample = pca_temp.fit_transform(sample_embeddings)\n",
    "\n",
    "    for k in test_ks:\n",
    "        kmeans_temp = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1000)\n",
    "        labels = kmeans_temp.fit_predict(pca_sample)\n",
    "        score = silhouette_score(pca_sample, labels)\n",
    "\n",
    "        results.append({\n",
    "            'n_components': n_comp,\n",
    "            'k': k,\n",
    "            'score': score\n",
    "        })\n",
    "\n",
    "        print(f\"  k={k}: score = {score:.4f}\")\n",
    "\n",
    "        if score > best_overall_score:\n",
    "            best_overall_score = score\n",
    "            best_components = n_comp\n",
    "            best_k = k\n",
    "\n",
    "print(f\"\\n=== BEST COMBINATION ===\")\n",
    "print(f\"Best n_components: {best_components}\")\n",
    "print(f\"Best k: {best_k}\")\n",
    "print(f\"Best silhouette score: {best_overall_score:.4f}\")\n",
    "\n",
    "# Set the optimal values\n",
    "optimal_components = best_components\n",
    "optimal_k = best_k\n",
    "\n",
    "# Import K-means\n",
    "\n",
    "\n",
    "kmeans_model = KMeans(\n",
    "    n_clusters=optimal_k,        # You need to specify number of clusters (start with 15-25)\n",
    "    random_state=42,      # For reproducibility\n",
    "    n_init=10,           # Number of random initializations\n",
    "    max_iter=300,        # Maximum iterations\n",
    "    algorithm='lloyd'    # Standard K-means algorithm\n",
    ")\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "# KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "# Part-of-Speech\n",
    "pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
    "\n",
    "# MMR\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# GPT-3.5\n",
    "prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
    "topic: <topic label>\n",
    "\"\"\"\n",
    "client = openai.OpenAI(api_key=\"\")\n",
    "openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n",
    "    \"MMR\": mmr_model,\n",
    "    \"POS\": pos_model\n",
    "}\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\n",
    "  # Pipeline models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=pca_model, #umap_model\n",
    "  hdbscan_model=kmeans_model, #hdbscan_model\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  representation_model=representation_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  top_n_words=20,\n",
    "  calculate_probabilities=True,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "#topics, probs = topic_model.fit_transform(docs=sentences, embeddings=final_embeddings_for_bertopic)\n",
    "topics, probs = topic_model.fit_transform(sentences, embeddings_numpy)\n",
    "topic_model.get_topic_info()\n",
    "# Step 2: Map sentence topics back to speech topics using original speech_id\n",
    "speech_topics_by_id = {}  # Use speech_id as key instead of index\n",
    "\n",
    "for sentence_idx, topic in enumerate(topics):\n",
    "    original_speech_id = sentence_mapping[sentence_idx][1]  # Get the original speech_id\n",
    "\n",
    "    if original_speech_id not in speech_topics_by_id:\n",
    "        speech_topics_by_id[original_speech_id] = []\n",
    "    speech_topics_by_id[original_speech_id].append(topic)\n",
    "\n",
    "# Step 3: Determine the most frequent topic for each speech by speech_id\n",
    "most_frequent_topics_by_id = {}\n",
    "\n",
    "for speech_id_val, topic_list in speech_topics_by_id.items():\n",
    "    topic_counts = pd.Series(topic_list).value_counts()\n",
    "    most_frequent_topic = topic_counts.index[0] if not topic_counts.empty else -1\n",
    "    most_frequent_topics_by_id[speech_id_val] = most_frequent_topic\n",
    "\n",
    "# Step 4: Create comprehensive mappings - topic to speech_ids, and topic+party to speech_ids\n",
    "\n",
    "# First, create a mapping from speech_id to party\n",
    "speech_id_to_party = {}\n",
    "for speech_idx in range(len(filtered_speeches)):\n",
    "    original_speech_id = filtered_speech_ids[speech_idx]\n",
    "    party = filtered_parties[speech_idx]\n",
    "    speech_id_to_party[original_speech_id] = party\n",
    "\n",
    "# Create the basic topic to speech_id mapping\n",
    "topic_to_speech_id = {}\n",
    "for speech_id_val, topic in most_frequent_topics_by_id.items():\n",
    "    if topic not in topic_to_speech_id:\n",
    "        topic_to_speech_id[topic] = []\n",
    "    topic_to_speech_id[topic].append(speech_id_val)\n",
    "\n",
    "# Create the enhanced mapping: topic + party to speech_ids\n",
    "topic_party_to_speech_id = {}\n",
    "for speech_id_val, topic in most_frequent_topics_by_id.items():\n",
    "    if speech_id_val in speech_id_to_party:\n",
    "        party = speech_id_to_party[speech_id_val]\n",
    "        key = (topic, party)\n",
    "\n",
    "        if key not in topic_party_to_speech_id:\n",
    "            topic_party_to_speech_id[key] = []\n",
    "        topic_party_to_speech_id[key].append(speech_id_val)\n",
    "\n",
    "# Helper functions to easily query the data\n",
    "def get_speech_ids_by_topic(topic_num):\n",
    "    \"\"\"Get all speech_ids for a given topic\"\"\"\n",
    "    return topic_to_speech_id.get(topic_num, [])\n",
    "\n",
    "def get_speech_ids_by_topic_and_party(topic_num, party):\n",
    "    \"\"\"Get speech_ids for a specific topic and party combination\"\"\"\n",
    "    return topic_party_to_speech_id.get((topic_num, party), [])\n",
    "\n",
    "def get_speech_ids_by_party_in_topic(topic_num):\n",
    "    \"\"\"Get speech_ids organized by party for a given topic\"\"\"\n",
    "    result = {}\n",
    "    all_speech_ids = topic_to_speech_id.get(topic_num, [])\n",
    "\n",
    "    for speech_id_val in all_speech_ids:\n",
    "        if speech_id_val in speech_id_to_party:\n",
    "            party = speech_id_to_party[speech_id_val]\n",
    "            if party not in result:\n",
    "                result[party] = []\n",
    "            result[party].append(speech_id_val)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Display sample results\n",
    "print(\"Sample topic to speech_id mapping:\")\n",
    "for topic_num in list(topic_to_speech_id.keys())[:5]:  # Show first 5 topics\n",
    "    print(f\"Topic {topic_num}: {len(topic_to_speech_id[topic_num])} speeches\")\n",
    "    print(f\"  First 5 speech_ids: {topic_to_speech_id[topic_num][:5]}\")\n",
    "\n",
    "print(\"\\nSample topic + party combinations:\")\n",
    "sample_keys = list(topic_party_to_speech_id.keys())[:10]\n",
    "for key in sample_keys:\n",
    "    topic_num, party = key\n",
    "    count = len(topic_party_to_speech_id[key])\n",
    "    print(f\"Topic {topic_num}, Party {party}: {count} speeches\")\n",
    "\n",
    "print(\"\\nExample usage:\")\n",
    "if len(topic_to_speech_id) > 0:\n",
    "    example_topic = list(topic_to_speech_id.keys())[1] if len(topic_to_speech_id) > 1 else list(topic_to_speech_id.keys())[0]\n",
    "\n",
    "    print(f\"\\nTopic {example_topic}:\")\n",
    "    print(f\"  All speech_ids: {len(get_speech_ids_by_topic(example_topic))} total\")\n",
    "\n",
    "    party_breakdown = get_speech_ids_by_party_in_topic(example_topic)\n",
    "    for party, speech_ids in party_breakdown.items():\n",
    "        print(f\"  {party}: {len(speech_ids)} speeches\")\n",
    "        print(f\"    Sample speech_ids: {speech_ids[:3]}\")\n",
    "\n",
    "    # Example of getting Republican speeches for this topic\n",
    "    if 'R' in party_breakdown:\n",
    "        republican_speeches = get_speech_ids_by_topic_and_party(example_topic, 'R')\n",
    "        print(f\"  Republican speeches in topic {example_topic}: {len(republican_speeches)}\")\n",
    "\n",
    "# If you need the topics_per_class functionality, create aligned arrays\n",
    "most_frequent_speech_topics_aligned = []\n",
    "filtered_speech_ids_aligned = []\n",
    "\n",
    "for speech_idx in range(len(filtered_speeches)):\n",
    "    original_speech_id = filtered_speech_ids[speech_idx]\n",
    "\n",
    "    if original_speech_id in most_frequent_topics_by_id:\n",
    "        topic = most_frequent_topics_by_id[original_speech_id]\n",
    "        most_frequent_speech_topics_aligned.append(topic)\n",
    "        filtered_speech_ids_aligned.append(original_speech_id)\n",
    "    else:\n",
    "        most_frequent_speech_topics_aligned.append(-1)\n",
    "        filtered_speech_ids_aligned.append(original_speech_id)\n",
    "\n",
    "print(f\"\\nAligned arrays length: {len(most_frequent_speech_topics_aligned)}\")\n",
    "print(f\"Filtered speeches length: {len(filtered_speeches)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"USAGE EXAMPLES:\")\n",
    "print(\"=\"*50)\n",
    "print(\"# Get all speech_ids for topic 1:\")\n",
    "print(\"speech_ids = get_speech_ids_by_topic(1)\")\n",
    "print(\"\\n# Get Republican speech_ids for topic 1:\")\n",
    "print(\"republican_speech_ids = get_speech_ids_by_topic_and_party(1, 'R')\")\n",
    "print(\"\\n# Get party breakdown for topic 1:\")\n",
    "print(\"party_breakdown = get_speech_ids_by_party_in_topic(1)\")\n",
    "print(\"\\n# Access the mappings directly:\")\n",
    "print(\"topic_to_speech_id[1]  # All speech_ids for topic 1\")\n",
    "print(\"topic_party_to_speech_id[(1, 'R')]  # Republican speech_ids for topic 1\")\n",
    "\n",
    "topics_per_party_df = topic_model.topics_per_class(filtered_speeches, classes=filtered_parties)\n",
    "print(\"Success! topics_per_class completed without error.\")\n",
    "print(topics_per_party_df)\n",
    "\n",
    "\n",
    "# or ChatGPT's labels\n",
    "chatgpt_topic_labels = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()}\n",
    "chatgpt_topic_labels[-1] = \"Outlier Topic\"\n",
    "topic_model.set_topic_labels(chatgpt_topic_labels)\n",
    "\n",
    "topic_model.visualize_topics(custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT ANALYSIS DATA FOR SUBSEQUENT ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \"/content/drive/MyDrive/congress-polarization-thesis/outputs/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. EXPORT TOPICS PER PARTY DATAFRAME WITH CUSTOM NAMES\n",
    "# ============================================================================\n",
    "\n",
    "# Get the topics_per_party dataframe\n",
    "topics_per_party_df = topic_model.topics_per_class(filtered_speeches, classes=filtered_parties)\n",
    "\n",
    "# Add custom topic names column\n",
    "# Extract the ChatGPT-generated labels that were set using set_topic_labels\n",
    "\n",
    "if hasattr(topic_model, 'custom_labels_') and topic_model.custom_labels_:\n",
    "    print(\"Found ChatGPT custom labels from set_topic_labels\")\n",
    "    custom_names = []\n",
    "    # Check if custom_labels_ is a list or dictionary\n",
    "    if isinstance(topic_model.custom_labels_, dict):\n",
    "        for topic_id in topics_per_party_df.index:\n",
    "            # Get the custom label, fallback to \"Topic X\" if not found\n",
    "            label = topic_model.custom_labels_.get(topic_id, f\"Topic {topic_id}\")\n",
    "            custom_names.append(label)\n",
    "    elif isinstance(topic_model.custom_labels_, list):\n",
    "         for topic_id in topics_per_party_df.index:\n",
    "            # Assuming the list is ordered by topic ID, access by index\n",
    "            if topic_id < len(topic_model.custom_labels_):\n",
    "                label = topic_model.custom_labels_[topic_id]\n",
    "            else:\n",
    "                label = f\"Topic {topic_id}\" # Fallback\n",
    "            custom_names.append(label)\n",
    "\n",
    "    topics_per_party_df['ChatGPT_Label'] = custom_names\n",
    "    print(f\"✓ Added ChatGPT labels to dataframe\")\n",
    "    print(f\"  Sample labels: {custom_names[:3]}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    # Fallback: try to recreate the labels from topic_aspects_ if available\n",
    "    if hasattr(topic_model, 'topic_aspects_') and 'OpenAI' in topic_model.topic_aspects_:\n",
    "        print(\"Recreating ChatGPT labels from topic_aspects_\")\n",
    "        chatgpt_topic_labels = {\n",
    "            topic: \" | \".join(list(zip(*values))[0])\n",
    "            for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()\n",
    "        }\n",
    "        chatgpt_topic_labels[-1] = \"Outlier Topic\"\n",
    "\n",
    "        custom_names = []\n",
    "        for topic_id in topics_per_party_df.index:\n",
    "            label = chatgpt_topic_labels.get(topic_id, f\"Topic {topic_id}\")\n",
    "            custom_names.append(label)\n",
    "\n",
    "        topics_per_party_df['ChatGPT_Label'] = custom_names\n",
    "        print(f\"✓ Recreated ChatGPT labels from topic_aspects_\")\n",
    "        print(f\"  Sample labels: {custom_names[:3]}\")\n",
    "\n",
    "    else:\n",
    "        # Final fallback: use default topic representations\n",
    "        print(\"No ChatGPT labels found - using default topic representations\")\n",
    "        custom_names = []\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        for topic_id in topics_per_party_df.index:\n",
    "            topic_row = topic_info[topic_info['Topic'] == topic_id]\n",
    "            if not topic_row.empty:\n",
    "                representation = topic_row['Representation'].iloc[0]\n",
    "                if isinstance(representation, list):\n",
    "                    label = \" | \".join(representation[:3])  # Match your format\n",
    "                else:\n",
    "                    label = str(representation)[:100]  # Longer truncation for readability\n",
    "                custom_names.append(label)\n",
    "            else:\n",
    "                custom_names.append(f\"Topic {topic_id}\")\n",
    "\n",
    "        topics_per_party_df['Default_Label'] = custom_names\n",
    "        print(f\"✓ Added default topic representations\")\n",
    "\n",
    "print(f\"Available columns in topics_per_party_df: {list(topics_per_party_df.columns)}\")\n",
    "\n",
    "# Export topics per party dataframe\n",
    "topics_csv_path = os.path.join(output_dir, \"topics_per_party_analysis.csv\")\n",
    "topics_per_party_df.to_csv(topics_csv_path, index=True)\n",
    "print(f\"✓ Exported topics per party dataframe to: {topics_csv_path}\")\n",
    "print(f\"  Shape: {topics_per_party_df.shape}\")\n",
    "print(f\"  Columns: {list(topics_per_party_df.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. EXPORT TOPIC-PARTY TO SPEECH_ID MAPPING\n",
    "# ============================================================================\n",
    "\n",
    "# Convert the topic_party_to_speech_id dictionary to a JSON-serializable format\n",
    "topic_party_mapping_json = {}\n",
    "for (topic, party), speech_ids in topic_party_to_speech_id.items():\n",
    "    key = f\"topic_{topic}_party_{party}\"\n",
    "    # Convert NumPy int64 to standard Python int\n",
    "    topic_party_mapping_json[key] = [int(speech_id) for speech_id in speech_ids]\n",
    "\n",
    "# Export topic-party to speech_id mapping\n",
    "mapping_json_path = os.path.join(output_dir, \"topic_party_to_speech_id_mapping.json\")\n",
    "with open(mapping_json_path, 'w') as f:\n",
    "    json.dump(topic_party_mapping_json, f, indent=2)\n",
    "print(f\"✓ Exported topic-party mapping to: {mapping_json_path}\")\n",
    "print(f\"  Total combinations: {len(topic_party_mapping_json)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. EXPORT ADDITIONAL USEFUL MAPPINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Export basic topic to speech_id mapping (without party)\n",
    "# Convert NumPy int64 to standard Python int\n",
    "topic_mapping_json = {str(topic): [int(speech_id) for speech_id in speech_ids] for topic, speech_ids in topic_to_speech_id.items()}\n",
    "topic_json_path = os.path.join(output_dir, \"topic_to_speech_id_mapping.json\")\n",
    "with open(topic_json_path, 'w') as f:\n",
    "    json.dump(topic_mapping_json, f, indent=2)\n",
    "print(f\"✓ Exported basic topic mapping to: {topic_json_path}\")\n",
    "\n",
    "# Export speech_id to party mapping\n",
    "# Convert NumPy int64 to standard Python int\n",
    "speech_party_json_path = os.path.join(output_dir, \"speech_id_to_party_mapping.json\")\n",
    "with open(speech_party_json_path, 'w') as f:\n",
    "    json.dump({int(speech_id): party for speech_id, party in speech_id_to_party.items()}, f, indent=2)\n",
    "print(f\"✓ Exported speech_id to party mapping to: {speech_party_json_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. EXPORT COMPREHENSIVE METADATA\n",
    "# ============================================================================\n",
    "\n",
    "# Create metadata about the analysis\n",
    "metadata = {\n",
    "    \"analysis_info\": {\n",
    "        \"congress_range\": f\"{congress_start}-{congress_end}\",\n",
    "        \"total_speeches_original\": len(speeches),\n",
    "        \"total_speeches_filtered\": len(filtered_speeches),\n",
    "        \"total_sentences\": len(sentences),\n",
    "        \"total_topics\": len(topic_to_speech_id),\n",
    "        \"parties\": list(set(filtered_parties)),\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"clustering_model\": \"KMeans\",\n",
    "        \"n_clusters\": optimal_k,  # Use the actual optimal_k value\n",
    "        \"representation_models\": list(topic_model.representation_model.keys()) if hasattr(topic_model, 'representation_model') else []\n",
    "    },\n",
    "    \"topic_summary\": {\n",
    "        # Convert NumPy int64 keys to strings for JSON compatibility\n",
    "        str(topic): {\n",
    "            \"total_speeches\": len(speech_ids),\n",
    "            \"parties\": list(get_speech_ids_by_party_in_topic(topic).keys()),\n",
    "            # Ensure party_counts values are standard Python ints\n",
    "            \"party_counts\": {party: int(len(party_speech_ids)) for party, party_speech_ids in get_speech_ids_by_party_in_topic(topic).items()}\n",
    "        }\n",
    "        for topic, speech_ids in topic_to_speech_id.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_json_path = os.path.join(output_dir, \"analysis_metadata.json\")\n",
    "with open(metadata_json_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"✓ Exported analysis metadata to: {metadata_json_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPORT COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"All files saved to: {output_dir}\")\n",
    "print(\"Files created:\")\n",
    "print(f\"  1. {topics_csv_path}\")\n",
    "print(f\"  2. {mapping_json_path}\")\n",
    "print(f\"  3. {topic_json_path}\")\n",
    "print(f\"  4. {speech_party_json_path}\")\n",
    "print(f\"  5. {metadata_json_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
